{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bceac7d2-eb00-45d1-aabd-d63a42f4e16b",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold;\"><u>Import et chargement des donnÃ©es</u></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368acf0e-0aa4-403e-a345-72d08166993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.stats import ks_2samp\n",
    "from myFonctions import (\n",
    "    get_cluster_labels,\n",
    "    compute_ari_between_periods,\n",
    "    compute_ks_test_between_periods,\n",
    "    plot_ari,\n",
    "    plot_feature_distribution\n",
    ")\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ecfb785-a16e-485b-b407-18b24472d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers la base de donnÃ©es SQLite\n",
    "db_path = 'data/olist.db'\n",
    "\n",
    "# CrÃ©er une connexion Ã  la base de donnÃ©es SQLite\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e970d743-4571-4317-a580-c48a8c4d3ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_pair</th>\n",
       "      <th>n_clients_common</th>\n",
       "      <th>ARI_kmeans</th>\n",
       "      <th>ARI_dbscan</th>\n",
       "      <th>KS_pvalue_recency</th>\n",
       "      <th>KS_pvalue_frequency</th>\n",
       "      <th>KS_pvalue_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017Q1 â†’ 2017Q2</td>\n",
       "      <td>48</td>\n",
       "      <td>0.028896</td>\n",
       "      <td>0.226282</td>\n",
       "      <td>1.790296e-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017Q2 â†’ 2017Q3</td>\n",
       "      <td>97</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.140347</td>\n",
       "      <td>7.071673e-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.963766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017Q3 â†’ 2017Q4</td>\n",
       "      <td>132</td>\n",
       "      <td>0.018704</td>\n",
       "      <td>0.246018</td>\n",
       "      <td>4.940724e-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.969610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017Q4 â†’ 2018Q1</td>\n",
       "      <td>137</td>\n",
       "      <td>0.028731</td>\n",
       "      <td>0.173639</td>\n",
       "      <td>1.088649e-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.860613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018Q1 â†’ 2018Q2</td>\n",
       "      <td>195</td>\n",
       "      <td>0.017444</td>\n",
       "      <td>0.125570</td>\n",
       "      <td>3.598142e-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018Q2 â†’ 2018Q3</td>\n",
       "      <td>102</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>6.693788e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       period_pair  n_clients_common  ARI_kmeans  ARI_dbscan  \\\n",
       "0  2017Q1 â†’ 2017Q2                48    0.028896    0.226282   \n",
       "1  2017Q2 â†’ 2017Q3                97    0.038491    0.140347   \n",
       "2  2017Q3 â†’ 2017Q4               132    0.018704    0.246018   \n",
       "3  2017Q4 â†’ 2018Q1               137    0.028731    0.173639   \n",
       "4  2018Q1 â†’ 2018Q2               195    0.017444    0.125570   \n",
       "5  2018Q2 â†’ 2018Q3               102    0.004690    0.038812   \n",
       "\n",
       "   KS_pvalue_recency  KS_pvalue_frequency  KS_pvalue_monetary  \n",
       "0       1.790296e-02                  1.0            0.692660  \n",
       "1       7.071673e-02                  1.0            0.963766  \n",
       "2       4.940724e-04                  1.0            0.969610  \n",
       "3       1.088649e-03                  1.0            0.860613  \n",
       "4       3.598142e-05                  1.0            0.081057  \n",
       "5       6.693788e-09                  1.0            0.994932  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Extraire les dates min/max\n",
    "query_min_max = \"\"\"\n",
    "SELECT MIN(order_purchase_timestamp) AS min_date,\n",
    "       MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders\n",
    "\"\"\"\n",
    "min_date, max_date = pd.read_sql_query(query_min_max, conn).iloc[0]\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = pd.to_datetime(max_date)\n",
    "\n",
    "# 2. GÃ©nÃ©rer tous les trimestres\n",
    "trimestres = pd.period_range(start=min_date, end=max_date, freq='Q')\n",
    "\n",
    "# 3. Identifier les trimestres avec au moins une commande\n",
    "df_dates = pd.read_sql_query(\"SELECT order_purchase_timestamp FROM orders\", conn)\n",
    "df_dates['order_purchase_timestamp'] = pd.to_datetime(df_dates['order_purchase_timestamp'])\n",
    "df_dates['period'] = df_dates['order_purchase_timestamp'].dt.to_period('Q')\n",
    "trimestres_utiles = df_dates['period'].drop_duplicates().sort_values()\n",
    "\n",
    "# 4. GÃ©nÃ©rer le RFM pour chaque trimestre\n",
    "rfm_all_periods = []\n",
    "\n",
    "for period in trimestres_utiles:\n",
    "    start = period.start_time\n",
    "    end = period.end_time\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT DATE('{end.date()}') AS global_max_date\n",
    "    ),\n",
    "    customer_orders AS (\n",
    "        SELECT \n",
    "            c.customer_unique_id,\n",
    "            o.order_id,\n",
    "            o.order_purchase_timestamp\n",
    "        FROM customers c\n",
    "        JOIN orders o ON c.customer_id = o.customer_id\n",
    "        WHERE o.order_purchase_timestamp BETWEEN '{start.date()}' AND '{end.date()}'\n",
    "    ),\n",
    "    payments AS (\n",
    "        SELECT \n",
    "            co.customer_unique_id,\n",
    "            MAX(co.order_purchase_timestamp) AS last_order_date,\n",
    "            COUNT(DISTINCT co.order_id) AS frequency,\n",
    "            SUM(op.payment_value) AS monetary\n",
    "        FROM customer_orders co\n",
    "        JOIN order_pymts op ON co.order_id = op.order_id\n",
    "        GROUP BY co.customer_unique_id\n",
    "    ),\n",
    "    rfm AS (\n",
    "        SELECT \n",
    "            p.customer_unique_id,\n",
    "            CAST(JULIANDAY(m.global_max_date) - JULIANDAY(p.last_order_date) AS INT) AS recency,\n",
    "            p.frequency,\n",
    "            ROUND(p.monetary, 2) AS monetary\n",
    "        FROM payments p\n",
    "        CROSS JOIN max_date m\n",
    "    )\n",
    "\n",
    "    SELECT *, '{period}' AS period\n",
    "    FROM rfm\n",
    "    \"\"\"\n",
    "\n",
    "    df_rfm = pd.read_sql_query(query, conn)\n",
    "    rfm_all_periods.append(df_rfm)\n",
    "\n",
    "rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n",
    "\n",
    "# 5. Calcul ARI et KS pour chaque paire de trimestres consÃ©cutifs\n",
    "periods = sorted(rfm_all_periods['period'].unique())\n",
    "results = []\n",
    "\n",
    "for i in range(len(periods) - 1):\n",
    "    period_1 = periods[i]\n",
    "    period_2 = periods[i + 1]\n",
    "\n",
    "    df1 = rfm_all_periods[rfm_all_periods['period'] == period_1].copy()\n",
    "    df2 = rfm_all_periods[rfm_all_periods['period'] == period_2].copy()\n",
    "\n",
    "    common_ids = set(df1.customer_unique_id).intersection(df2.customer_unique_id)\n",
    "    df1 = df1[df1.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "    df2 = df2[df2.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "\n",
    "    if len(common_ids) < 5:\n",
    "        continue  # ignorer si trop peu de clients communs\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(df1[['recency', 'frequency', 'monetary']])\n",
    "    X2 = scaler.fit_transform(df2[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "    kmeans_1 = KMeans(n_clusters=4, random_state=42).fit(X1)\n",
    "    kmeans_2 = KMeans(n_clusters=4, random_state=42).fit(X2)\n",
    "\n",
    "    dbscan_1 = DBSCAN(eps=1.0, min_samples=5).fit(X1)\n",
    "    dbscan_2 = DBSCAN(eps=1.0, min_samples=5).fit(X2)\n",
    "\n",
    "    ari_kmeans = adjusted_rand_score(kmeans_1.labels_, kmeans_2.labels_)\n",
    "    ari_dbscan = adjusted_rand_score(dbscan_1.labels_, dbscan_2.labels_)\n",
    "\n",
    "    ks_results = {\n",
    "        'recency': ks_2samp(df1['recency'], df2['recency']).pvalue,\n",
    "        'frequency': ks_2samp(df1['frequency'], df2['frequency']).pvalue,\n",
    "        'monetary': ks_2samp(df1['monetary'], df2['monetary']).pvalue,\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'period_pair': f\"{period_1} â†’ {period_2}\",\n",
    "        'n_clients_common': len(common_ids),\n",
    "        'ARI_kmeans': ari_kmeans,\n",
    "        'ARI_dbscan': ari_dbscan,\n",
    "        'KS_pvalue_recency': ks_results['recency'],\n",
    "        'KS_pvalue_frequency': ks_results['frequency'],\n",
    "        'KS_pvalue_monetary': ks_results['monetary'],\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(results)\n",
    "display(df_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e578c-0fd5-4f7b-a661-946ab03b04d1",
   "metadata": {},
   "source": [
    "### ðŸ“Š RÃ©sumÃ© des indicateurs d'Ã©volution des clusters (RFM trimestriel)\n",
    "\n",
    "| Indicateur             | RÃ©sultat observÃ©                  | InterprÃ©tation                                                                 |\n",
    "|------------------------|-----------------------------------|--------------------------------------------------------------------------------|\n",
    "| **ARI - KMeans**       | TrÃ¨s faible (max â‰ˆ 0.04)          | Les clients changent frÃ©quemment de groupe, **clustering instable**.          |\n",
    "| **ARI - DBSCAN**       | Faible Ã  modÃ©rÃ© (0.04 Ã  0.25)     | Un peu plus robuste que KMeans, mais **reste instable dans le temps**.        |\n",
    "| **KS Test â€“ Recency**  | p-values souvent < 0.05           | **Variation significative** dans la rÃ©cence : les clients changent d'activitÃ©. |\n",
    "| **KS Test â€“ Frequency**| p-values â‰ˆ 1.0                    | **Stable** : les clients achÃ¨tent avec une frÃ©quence similaire.               |\n",
    "| **KS Test â€“ Monetary** | p-values Ã©levÃ©es (souvent > 0.6)  | **Stable** : les montants dÃ©pensÃ©s restent globalement constants.             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab02d4-a1af-4b54-ab89-3d288aed8be2",
   "metadata": {},
   "source": [
    "ðŸ“Œ Analyse stratÃ©gique : passer Ã  une analyse mensuelle ?\n",
    "Les rÃ©sultats montrent que :\n",
    "\n",
    "Les comportements de frÃ©quence et montant sont stables,\n",
    "\n",
    "Mais les clusters restent instables Ã  cause de changements frÃ©quents dans la rÃ©cence, ce qui affecte fortement le RFM global.\n",
    "\n",
    "Cela soulÃ¨ve une question cruciale :\n",
    "ðŸ‘‰ Est-ce que lâ€™agrÃ©gation trimestrielle est trop grossiÃ¨re pour capturer les dynamiques comportementales des clients ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689278f-bacd-4178-9fc8-6b848670e82d",
   "metadata": {},
   "source": [
    "âœ… Pourquoi tester le RFM tous les mois ?\n",
    "\n",
    "Argument\tAvantage attendu\n",
    "Plus grande granularitÃ© temporelle\tPermet de dÃ©tecter plus tÃ´t les changements dâ€™habitude.\n",
    "Meilleure dÃ©tection des Ã©vÃ©nements rÃ©cents\tUtile si le comportement client varie rapidement (ex : promotions, saisons).\n",
    "Peut rÃ©vÃ©ler des micro-segments plus cohÃ©rents\tCertains segments peuvent Ãªtre stables Ã  court terme, mais invisibles Ã  lâ€™Ã©chelle trimestrielle.\n",
    "Suivi plus prÃ©cis de la rÃ©cence\tComme câ€™est la dimension la plus instable, elle sera mieux mesurÃ©e mensuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9792196-3c58-4fcc-a415-d86c96694b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_pair</th>\n",
       "      <th>n_clients_common</th>\n",
       "      <th>ARI_kmeans</th>\n",
       "      <th>ARI_dbscan</th>\n",
       "      <th>KS_pvalue_recency</th>\n",
       "      <th>KS_pvalue_frequency</th>\n",
       "      <th>KS_pvalue_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03 â†’ 2017-04</td>\n",
       "      <td>13</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04 â†’ 2017-05</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.086567</td>\n",
       "      <td>-0.136553</td>\n",
       "      <td>0.635485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05 â†’ 2017-06</td>\n",
       "      <td>18</td>\n",
       "      <td>0.070729</td>\n",
       "      <td>0.061350</td>\n",
       "      <td>0.132394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-06 â†’ 2017-07</td>\n",
       "      <td>15</td>\n",
       "      <td>0.021253</td>\n",
       "      <td>0.184994</td>\n",
       "      <td>0.678138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07 â†’ 2017-08</td>\n",
       "      <td>23</td>\n",
       "      <td>0.076579</td>\n",
       "      <td>0.072168</td>\n",
       "      <td>0.024720</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.421782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-08 â†’ 2017-09</td>\n",
       "      <td>31</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>-0.073979</td>\n",
       "      <td>0.079120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-09 â†’ 2017-10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.089063</td>\n",
       "      <td>0.096886</td>\n",
       "      <td>0.392945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-10 â†’ 2017-11</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.010073</td>\n",
       "      <td>-0.064352</td>\n",
       "      <td>0.706867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.510011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-11 â†’ 2017-12</td>\n",
       "      <td>40</td>\n",
       "      <td>0.132505</td>\n",
       "      <td>0.153288</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-12 â†’ 2018-01</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.084507</td>\n",
       "      <td>-0.118145</td>\n",
       "      <td>0.537929</td>\n",
       "      <td>0.978078</td>\n",
       "      <td>0.537929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01 â†’ 2018-02</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.004552</td>\n",
       "      <td>-0.100806</td>\n",
       "      <td>0.935662</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.753697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-02 â†’ 2018-03</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.070781</td>\n",
       "      <td>-0.009443</td>\n",
       "      <td>0.124295</td>\n",
       "      <td>0.992377</td>\n",
       "      <td>0.421782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-03 â†’ 2018-04</td>\n",
       "      <td>34</td>\n",
       "      <td>0.316368</td>\n",
       "      <td>0.233816</td>\n",
       "      <td>0.672684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-04 â†’ 2018-05</td>\n",
       "      <td>44</td>\n",
       "      <td>0.083338</td>\n",
       "      <td>-0.053163</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.318829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-05 â†’ 2018-06</td>\n",
       "      <td>38</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>-0.061147</td>\n",
       "      <td>0.737879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-06 â†’ 2018-07</td>\n",
       "      <td>21</td>\n",
       "      <td>0.179783</td>\n",
       "      <td>0.101604</td>\n",
       "      <td>0.602813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-07 â†’ 2018-08</td>\n",
       "      <td>48</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>0.497903</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.960172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-08 â†’ 2018-09</td>\n",
       "      <td>8</td>\n",
       "      <td>0.269565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "      <td>0.980109</td>\n",
       "      <td>0.980109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          period_pair  n_clients_common  ARI_kmeans  ARI_dbscan  \\\n",
       "0   2017-03 â†’ 2017-04                13    0.014440    0.082353   \n",
       "1   2017-04 â†’ 2017-05                14   -0.086567   -0.136553   \n",
       "2   2017-05 â†’ 2017-06                18    0.070729    0.061350   \n",
       "3   2017-06 â†’ 2017-07                15    0.021253    0.184994   \n",
       "4   2017-07 â†’ 2017-08                23    0.076579    0.072168   \n",
       "5   2017-08 â†’ 2017-09                31    0.048171   -0.073979   \n",
       "6   2017-09 â†’ 2017-10                30    0.089063    0.096886   \n",
       "7   2017-10 â†’ 2017-11                36   -0.010073   -0.064352   \n",
       "8   2017-11 â†’ 2017-12                40    0.132505    0.153288   \n",
       "9   2017-12 â†’ 2018-01                19   -0.084507   -0.118145   \n",
       "10  2018-01 â†’ 2018-02                27   -0.004552   -0.100806   \n",
       "11  2018-02 â†’ 2018-03                23   -0.070781   -0.009443   \n",
       "12  2018-03 â†’ 2018-04                34    0.316368    0.233816   \n",
       "13  2018-04 â†’ 2018-05                44    0.083338   -0.053163   \n",
       "14  2018-05 â†’ 2018-06                38    0.008463   -0.061147   \n",
       "15  2018-06 â†’ 2018-07                21    0.179783    0.101604   \n",
       "16  2018-07 â†’ 2018-08                48    0.034450    0.497903   \n",
       "17  2018-08 â†’ 2018-09                 8    0.269565    1.000000   \n",
       "\n",
       "    KS_pvalue_recency  KS_pvalue_frequency  KS_pvalue_monetary  \n",
       "0            0.126488             1.000000            0.999212  \n",
       "1            0.635485             1.000000            0.999592  \n",
       "2            0.132394             1.000000            0.971540  \n",
       "3            0.678138             1.000000            0.938331  \n",
       "4            0.024720             0.999999            0.421782  \n",
       "5            0.079120             1.000000            0.823454  \n",
       "6            0.392945             1.000000            0.807963  \n",
       "7            0.706867             1.000000            0.510011  \n",
       "8            0.000066             1.000000            0.918805  \n",
       "9            0.537929             0.978078            0.537929  \n",
       "10           0.935662             1.000000            0.753697  \n",
       "11           0.124295             0.992377            0.421782  \n",
       "12           0.672684             1.000000            0.672684  \n",
       "13           0.011526             1.000000            0.318829  \n",
       "14           0.737879             1.000000            0.903253  \n",
       "15           0.602813             1.000000            0.987044  \n",
       "16           0.000956             0.999992            0.960172  \n",
       "17           0.660140             0.980109            0.980109  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"ks_2samp: Exact calculation unsuccessful.*\")\n",
    "# 1. Extraire les dates min/max\n",
    "query_min_max = \"\"\"\n",
    "SELECT MIN(order_purchase_timestamp) AS min_date,\n",
    "       MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders\n",
    "\"\"\"\n",
    "min_date, max_date = pd.read_sql_query(query_min_max, conn).iloc[0]\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = pd.to_datetime(max_date)\n",
    "\n",
    "# 2. GÃ©nÃ©rer tous les mois\n",
    "mois = pd.period_range(start=min_date, end=max_date, freq='M')\n",
    "\n",
    "# 3. Identifier les mois avec au moins une commande\n",
    "df_dates = pd.read_sql_query(\"SELECT order_purchase_timestamp FROM orders\", conn)\n",
    "df_dates['order_purchase_timestamp'] = pd.to_datetime(df_dates['order_purchase_timestamp'])\n",
    "df_dates['period'] = df_dates['order_purchase_timestamp'].dt.to_period('M')\n",
    "mois_utiles = df_dates['period'].drop_duplicates().sort_values()\n",
    "\n",
    "# 4. GÃ©nÃ©rer le RFM pour chaque mois\n",
    "rfm_all_periods = []\n",
    "\n",
    "for period in mois_utiles:\n",
    "    start = period.start_time\n",
    "    end = period.end_time\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT DATE('{end.date()}') AS global_max_date\n",
    "    ),\n",
    "    customer_orders AS (\n",
    "        SELECT \n",
    "            c.customer_unique_id,\n",
    "            o.order_id,\n",
    "            o.order_purchase_timestamp\n",
    "        FROM customers c\n",
    "        JOIN orders o ON c.customer_id = o.customer_id\n",
    "        WHERE o.order_purchase_timestamp BETWEEN '{start.date()}' AND '{end.date()}'\n",
    "    ),\n",
    "    payments AS (\n",
    "        SELECT \n",
    "            co.customer_unique_id,\n",
    "            MAX(co.order_purchase_timestamp) AS last_order_date,\n",
    "            COUNT(DISTINCT co.order_id) AS frequency,\n",
    "            SUM(op.payment_value) AS monetary\n",
    "        FROM customer_orders co\n",
    "        JOIN order_pymts op ON co.order_id = op.order_id\n",
    "        GROUP BY co.customer_unique_id\n",
    "    ),\n",
    "    rfm AS (\n",
    "        SELECT \n",
    "            p.customer_unique_id,\n",
    "            CAST(JULIANDAY(m.global_max_date) - JULIANDAY(p.last_order_date) AS INT) AS recency,\n",
    "            p.frequency,\n",
    "            ROUND(p.monetary, 2) AS monetary\n",
    "        FROM payments p\n",
    "        CROSS JOIN max_date m\n",
    "    )\n",
    "\n",
    "    SELECT *, '{period}' AS period\n",
    "    FROM rfm\n",
    "    \"\"\"\n",
    "\n",
    "    df_rfm = pd.read_sql_query(query, conn)\n",
    "    rfm_all_periods.append(df_rfm)\n",
    "\n",
    "rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n",
    "\n",
    "# 5. Calcul ARI et KS pour chaque paire de mois consÃ©cutifs\n",
    "periods = sorted(rfm_all_periods['period'].unique())\n",
    "results = []\n",
    "\n",
    "for i in range(len(periods) - 1):\n",
    "    period_1 = periods[i]\n",
    "    period_2 = periods[i + 1]\n",
    "\n",
    "    df1 = rfm_all_periods[rfm_all_periods['period'] == period_1].copy()\n",
    "    df2 = rfm_all_periods[rfm_all_periods['period'] == period_2].copy()\n",
    "\n",
    "    common_ids = set(df1.customer_unique_id).intersection(df2.customer_unique_id)\n",
    "    df1 = df1[df1.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "    df2 = df2[df2.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "\n",
    "    if len(common_ids) < 5:\n",
    "        continue  # ignorer si trop peu de clients communs\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(df1[['recency', 'frequency', 'monetary']])\n",
    "    X2 = scaler.fit_transform(df2[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "    kmeans_1 = KMeans(n_clusters=4, random_state=42).fit(X1)\n",
    "    kmeans_2 = KMeans(n_clusters=4, random_state=42).fit(X2)\n",
    "\n",
    "    dbscan_1 = DBSCAN(eps=1.0, min_samples=5).fit(X1)\n",
    "    dbscan_2 = DBSCAN(eps=1.0, min_samples=5).fit(X2)\n",
    "\n",
    "    ari_kmeans = adjusted_rand_score(kmeans_1.labels_, kmeans_2.labels_)\n",
    "    ari_dbscan = adjusted_rand_score(dbscan_1.labels_, dbscan_2.labels_)\n",
    "\n",
    "    ks_results = {\n",
    "        'recency': ks_2samp(df1['recency'], df2['recency']).pvalue,\n",
    "        'frequency': ks_2samp(df1['frequency'], df2['frequency']).pvalue,\n",
    "        'monetary': ks_2samp(df1['monetary'], df2['monetary']).pvalue,\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'period_pair': f\"{period_1} â†’ {period_2}\",\n",
    "        'n_clients_common': len(common_ids),\n",
    "        'ARI_kmeans': ari_kmeans,\n",
    "        'ARI_dbscan': ari_dbscan,\n",
    "        'KS_pvalue_recency': ks_results['recency'],\n",
    "        'KS_pvalue_frequency': ks_results['frequency'],\n",
    "        'KS_pvalue_monetary': ks_results['monetary'],\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(results)\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ebac8-a262-463a-9d47-b4d263958768",
   "metadata": {},
   "source": [
    "| PÃ©riode               | Clients Communs | ARI KMeans | ARI DBSCAN | RFM Change ?      | KMeans Stable ? | DBSCAN Stable ? | Conclusion SynthÃ©tique                                       |\n",
    "|-----------------------|------------------|------------|------------|-------------------|------------------|------------------|----------------------------------------------------------------|\n",
    "| 2017-03 â†’ 2017-04     | 13               | 0.014      | 0.082      | ðŸŸ¢ Stable         | âŒ Faible        | âŒ Faible        | Clusters trÃ¨s instables, pas de changement RFM                |\n",
    "| 2017-04 â†’ 2017-05     | 14               | -0.087     | -0.137     | ðŸŸ¢ Stable         | âŒ TrÃ¨s faible   | âŒ TrÃ¨s faible   | InstabilitÃ© des deux modÃ¨les sans Ã©volution comportementale    |\n",
    "| 2017-05 â†’ 2017-06     | 18               | 0.071      | 0.061      | ðŸŸ¢ Stable         | âš ï¸ Moyenne       | âš ï¸ Moyenne       | LÃ©gÃ¨re stabilitÃ©, comportement clients constant                |\n",
    "| 2017-06 â†’ 2017-07     | 15               | 0.021      | 0.185      | ðŸŸ¢ Stable         | âŒ Faible        | âš ï¸ Moyenne       | DBSCAN un peu plus cohÃ©rent que KMeans                        |\n",
    "| 2017-07 â†’ 2017-08     | 23               | 0.077      | 0.072      | ðŸ”¥ Recency        | âš ï¸ Moyenne       | âš ï¸ Moyenne       | DÃ©but de changement RFM, lÃ©gers ajustements de cluster        |\n",
    "| 2017-08 â†’ 2017-09     | 31               | 0.048      | -0.074     | ðŸŸ¢ Stable         | âš ï¸ Faible        | âŒ TrÃ¨s faible   | Faible cohÃ©rence, surtout DBSCAN                             |\n",
    "| 2017-09 â†’ 2017-10     | 30               | 0.089      | 0.097      | ðŸŸ¢ Stable         | âš ï¸ Moyenne       | âš ï¸ Moyenne       | StabilitÃ© modÃ©rÃ©e dans les deux approches                     |\n",
    "| 2017-10 â†’ 2017-11     | 36               | -0.010     | -0.064     | ðŸŸ¢ Stable         | âŒ TrÃ¨s faible   | âŒ TrÃ¨s faible   | InstabilitÃ© injustifiÃ©e dans les deux modÃ¨les                 |\n",
    "| 2017-11 â†’ 2017-12     | 40               | 0.133      | 0.153      | ðŸ”¥ Recency        | âš ï¸ Moyenne       | âš ï¸ Moyenne       | RÃ©cence change, stabilitÃ© moyenne                            |\n",
    "| 2017-12 â†’ 2018-01     | 19               | -0.085     | -0.118     | ðŸŸ¢ Stable         | âŒ Faible        | âŒ Faible        | Cluster incohÃ©rent sans changement de fond                    |\n",
    "| 2018-01 â†’ 2018-02     | 27               | -0.005     | -0.101     | ðŸŸ¢ Stable         | âŒ Faible        | âŒ Faible        | Faible stabilitÃ© malgrÃ© comportement stable                   |\n",
    "| 2018-02 â†’ 2018-03     | 23               | -0.071     | -0.009     | ðŸŸ¢ Stable         | âŒ Faible        | âš ï¸ Moyenne       | LÃ©ger mieux pour DBSCAN, mais pas convaincant                 |\n",
    "| 2018-03 â†’ 2018-04     | 34               | 0.316      | 0.234      | ðŸŸ¢ Stable         | âœ… Bonne         | âœ… Bonne         | Excellente cohÃ©rence dans les deux modÃ¨les                    |\n",
    "| 2018-04 â†’ 2018-05     | 44               | 0.083      | -0.053     | ðŸ”¥ Recency        | âš ï¸ Moyenne       | âŒ Faible        | DBSCAN perd en cohÃ©rence malgrÃ© rupture rÃ©cence              |\n",
    "| 2018-05 â†’ 2018-06     | 38               | 0.008      | -0.061     | ðŸŸ¢ Stable         | âŒ Faible        | âŒ Faible        | IncohÃ©rence persistante, pas de comportement nouveau          |\n",
    "| 2018-06 â†’ 2018-07     | 21               | 0.180      | 0.102      | ðŸŸ¢ Stable         | âš ï¸ Moyenne       | âš ï¸ Moyenne       | Meilleure tenue des clusters, stabilitÃ© comportementale       |\n",
    "| 2018-07 â†’ 2018-08     | 48               | 0.034      | 0.498      | ðŸ”¥ Recency        | âŒ Faible        | âœ… Bonne         | DBSCAN rÃ©agit mieux Ã  la rupture comportementale              |\n",
    "| 2018-08 â†’ 2018-09     | 8                | 0.270      | 1.000      | ðŸŸ¢ Stable         | âœ… Bonne         | âœ… Excellente     | TrÃ¨s forte stabilitÃ©, mais effectif trop faible pour conclure |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa0fb1d-e440-44f4-b899-12530d26aaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_pair</th>\n",
       "      <th>n_clients_common</th>\n",
       "      <th>ARI_kmeans</th>\n",
       "      <th>ARI_dbscan</th>\n",
       "      <th>KS_pvalue_recency</th>\n",
       "      <th>KS_pvalue_frequency</th>\n",
       "      <th>KS_pvalue_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-01 â†’ 2017-02-16 â†’ 2017-02-16 â†’ 2017-03-03</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-03-18 â†’ 2017-04-02 â†’ 2017-04-02 â†’ 2017-04-17</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-02 â†’ 2017-04-17 â†’ 2017-04-17 â†’ 2017-05-02</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-17 â†’ 2017-05-02 â†’ 2017-05-02 â†’ 2017-05-17</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575175</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.962704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-02 â†’ 2017-05-17 â†’ 2017-05-17 â†’ 2017-06-01</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.020243</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.125874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-05-17 â†’ 2017-06-01 â†’ 2017-06-01 â†’ 2017-06-16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-06-01 â†’ 2017-06-16 â†’ 2017-06-16 â†’ 2017-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-07-01 â†’ 2017-07-16 â†’ 2017-07-16 â†’ 2017-07-31</td>\n",
       "      <td>12</td>\n",
       "      <td>0.030651</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.099547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-07-16 â†’ 2017-07-31 â†’ 2017-07-31 â†’ 2017-08-15</td>\n",
       "      <td>11</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-07-31 â†’ 2017-08-15 â†’ 2017-08-15 â†’ 2017-08-30</td>\n",
       "      <td>9</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-08-30 â†’ 2017-09-14 â†’ 2017-09-14 â†’ 2017-09-29</td>\n",
       "      <td>9</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-09-14 â†’ 2017-09-29 â†’ 2017-09-29 â†’ 2017-10-14</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.153846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-09-29 â†’ 2017-10-14 â†’ 2017-10-14 â†’ 2017-10-29</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.351707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-10-14 â†’ 2017-10-29 â†’ 2017-10-29 â†’ 2017-11-13</td>\n",
       "      <td>10</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-10-29 â†’ 2017-11-13 â†’ 2017-11-13 â†’ 2017-11-28</td>\n",
       "      <td>12</td>\n",
       "      <td>0.102285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-11-13 â†’ 2017-11-28 â†’ 2017-11-28 â†’ 2017-12-13</td>\n",
       "      <td>28</td>\n",
       "      <td>0.136773</td>\n",
       "      <td>0.193483</td>\n",
       "      <td>0.010890</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-11-28 â†’ 2017-12-13 â†’ 2017-12-13 â†’ 2017-12-28</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-12-13 â†’ 2017-12-28 â†’ 2017-12-28 â†’ 2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-12-28 â†’ 2018-01-12 â†’ 2018-01-12 â†’ 2018-01-27</td>\n",
       "      <td>14</td>\n",
       "      <td>0.151399</td>\n",
       "      <td>-0.060909</td>\n",
       "      <td>0.343320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.635485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-12 â†’ 2018-01-27 â†’ 2018-01-27 â†’ 2018-02-11</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-02-11 â†’ 2018-02-26 â†’ 2018-02-26 â†’ 2018-03-13</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.235294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.575175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-02-26 â†’ 2018-03-13 â†’ 2018-03-13 â†’ 2018-03-28</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.116505</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-03-13 â†’ 2018-03-28 â†’ 2018-03-28 â†’ 2018-04-12</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.007179</td>\n",
       "      <td>-0.060909</td>\n",
       "      <td>0.920516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-03-28 â†’ 2018-04-12 â†’ 2018-04-12 â†’ 2018-04-27</td>\n",
       "      <td>8</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-04-12 â†’ 2018-04-27 â†’ 2018-04-27 â†’ 2018-05-12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-04-27 â†’ 2018-05-12 â†’ 2018-05-12 â†’ 2018-05-27</td>\n",
       "      <td>12</td>\n",
       "      <td>0.398714</td>\n",
       "      <td>-0.079019</td>\n",
       "      <td>0.536098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-05-12 â†’ 2018-05-27 â†’ 2018-05-27 â†’ 2018-06-11</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.011111</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.343320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-05-27 â†’ 2018-06-11 â†’ 2018-06-11 â†’ 2018-06-26</td>\n",
       "      <td>11</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>-0.026667</td>\n",
       "      <td>0.832588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-06-11 â†’ 2018-06-26 â†’ 2018-06-26 â†’ 2018-07-11</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-06-26 â†’ 2018-07-11 â†’ 2018-07-11 â†’ 2018-07-26</td>\n",
       "      <td>11</td>\n",
       "      <td>0.048443</td>\n",
       "      <td>-0.078431</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-07-11 â†’ 2018-07-26 â†’ 2018-07-26 â†’ 2018-08-10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>0.602510</td>\n",
       "      <td>0.571336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-07-26 â†’ 2018-08-10 â†’ 2018-08-10 â†’ 2018-08-25</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.030295</td>\n",
       "      <td>0.286689</td>\n",
       "      <td>0.632386</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.990057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-08-10 â†’ 2018-08-25 â†’ 2018-08-25 â†’ 2018-09-09</td>\n",
       "      <td>9</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          period_pair  n_clients_common  \\\n",
       "0   2017-02-01 â†’ 2017-02-16 â†’ 2017-02-16 â†’ 2017-03-03                 8   \n",
       "1   2017-03-18 â†’ 2017-04-02 â†’ 2017-04-02 â†’ 2017-04-17                 5   \n",
       "2   2017-04-02 â†’ 2017-04-17 â†’ 2017-04-17 â†’ 2017-05-02                 5   \n",
       "3   2017-04-17 â†’ 2017-05-02 â†’ 2017-05-02 â†’ 2017-05-17                 7   \n",
       "4   2017-05-02 â†’ 2017-05-17 â†’ 2017-05-17 â†’ 2017-06-01                 9   \n",
       "5   2017-05-17 â†’ 2017-06-01 â†’ 2017-06-01 â†’ 2017-06-16                 6   \n",
       "6   2017-06-01 â†’ 2017-06-16 â†’ 2017-06-16 â†’ 2017-07-01                 5   \n",
       "7   2017-07-01 â†’ 2017-07-16 â†’ 2017-07-16 â†’ 2017-07-31                12   \n",
       "8   2017-07-16 â†’ 2017-07-31 â†’ 2017-07-31 â†’ 2017-08-15                11   \n",
       "9   2017-07-31 â†’ 2017-08-15 â†’ 2017-08-15 â†’ 2017-08-30                 9   \n",
       "10  2017-08-30 â†’ 2017-09-14 â†’ 2017-09-14 â†’ 2017-09-29                 9   \n",
       "11  2017-09-14 â†’ 2017-09-29 â†’ 2017-09-29 â†’ 2017-10-14                 6   \n",
       "12  2017-09-29 â†’ 2017-10-14 â†’ 2017-10-14 â†’ 2017-10-29                 9   \n",
       "13  2017-10-14 â†’ 2017-10-29 â†’ 2017-10-29 â†’ 2017-11-13                10   \n",
       "14  2017-10-29 â†’ 2017-11-13 â†’ 2017-11-13 â†’ 2017-11-28                12   \n",
       "15  2017-11-13 â†’ 2017-11-28 â†’ 2017-11-28 â†’ 2017-12-13                28   \n",
       "16  2017-11-28 â†’ 2017-12-13 â†’ 2017-12-13 â†’ 2017-12-28                 6   \n",
       "17  2017-12-13 â†’ 2017-12-28 â†’ 2017-12-28 â†’ 2018-01-12                 5   \n",
       "18  2017-12-28 â†’ 2018-01-12 â†’ 2018-01-12 â†’ 2018-01-27                14   \n",
       "19  2018-01-12 â†’ 2018-01-27 â†’ 2018-01-27 â†’ 2018-02-11                 8   \n",
       "20  2018-02-11 â†’ 2018-02-26 â†’ 2018-02-26 â†’ 2018-03-13                 7   \n",
       "21  2018-02-26 â†’ 2018-03-13 â†’ 2018-03-13 â†’ 2018-03-28                11   \n",
       "22  2018-03-13 â†’ 2018-03-28 â†’ 2018-03-28 â†’ 2018-04-12                14   \n",
       "23  2018-03-28 â†’ 2018-04-12 â†’ 2018-04-12 â†’ 2018-04-27                 8   \n",
       "24  2018-04-12 â†’ 2018-04-27 â†’ 2018-04-27 â†’ 2018-05-12                10   \n",
       "25  2018-04-27 â†’ 2018-05-12 â†’ 2018-05-12 â†’ 2018-05-27                12   \n",
       "26  2018-05-12 â†’ 2018-05-27 â†’ 2018-05-27 â†’ 2018-06-11                14   \n",
       "27  2018-05-27 â†’ 2018-06-11 â†’ 2018-06-11 â†’ 2018-06-26                11   \n",
       "28  2018-06-11 â†’ 2018-06-26 â†’ 2018-06-26 â†’ 2018-07-11                 5   \n",
       "29  2018-06-26 â†’ 2018-07-11 â†’ 2018-07-11 â†’ 2018-07-26                11   \n",
       "30  2018-07-11 â†’ 2018-07-26 â†’ 2018-07-26 â†’ 2018-08-10                20   \n",
       "31  2018-07-26 â†’ 2018-08-10 â†’ 2018-08-10 â†’ 2018-08-25                22   \n",
       "32  2018-08-10 â†’ 2018-08-25 â†’ 2018-08-25 â†’ 2018-09-09                 9   \n",
       "\n",
       "    ARI_kmeans  ARI_dbscan  KS_pvalue_recency  KS_pvalue_frequency  \\\n",
       "0     0.026087    1.000000           0.660140             1.000000   \n",
       "1    -0.111111    1.000000           1.000000             1.000000   \n",
       "2    -0.111111    1.000000           0.357143             1.000000   \n",
       "3    -0.037037    1.000000           0.575175             0.999961   \n",
       "4    -0.020243   -0.012500           0.125874             1.000000   \n",
       "5     0.285714    1.000000           0.474026             1.000000   \n",
       "6    -0.111111    1.000000           1.000000             1.000000   \n",
       "7     0.030651    0.022222           0.099547             1.000000   \n",
       "8     0.036558    0.000000           0.479150             1.000000   \n",
       "9     0.172414    0.000000           0.351707             1.000000   \n",
       "10    0.428571    0.555556           0.006294             1.000000   \n",
       "11   -0.153846    1.000000           0.142857             1.000000   \n",
       "12   -0.185185    0.000000           0.989469             1.000000   \n",
       "13    0.123077    0.000000           0.417524             1.000000   \n",
       "14    0.102285    0.000000           0.868982             1.000000   \n",
       "15    0.136773    0.193483           0.010890             1.000000   \n",
       "16   -0.250000    1.000000           0.474026             1.000000   \n",
       "17   -0.111111    1.000000           0.873016             1.000000   \n",
       "18    0.151399   -0.060909           0.343320             1.000000   \n",
       "19   -0.083333    1.000000           0.660140             1.000000   \n",
       "20   -0.235294    1.000000           0.212121             0.999961   \n",
       "21   -0.116505   -0.100000           0.479150             1.000000   \n",
       "22   -0.007179   -0.060909           0.920516             1.000000   \n",
       "23    0.151515    1.000000           0.660140             1.000000   \n",
       "24    0.166667    0.000000           0.786930             1.000000   \n",
       "25    0.398714   -0.079019           0.536098             1.000000   \n",
       "26   -0.011111    0.048338           0.343320             1.000000   \n",
       "27    0.226804   -0.026667           0.832588             1.000000   \n",
       "28   -0.111111    1.000000           0.357143             1.000000   \n",
       "29    0.048443   -0.078431           0.479150             1.000000   \n",
       "30    0.009324    0.602510           0.571336             1.000000   \n",
       "31   -0.030295    0.286689           0.632386             0.999998   \n",
       "32    0.520000    1.000000           0.730111             1.000000   \n",
       "\n",
       "    KS_pvalue_monetary  \n",
       "0             0.660140  \n",
       "1             1.000000  \n",
       "2             1.000000  \n",
       "3             0.962704  \n",
       "4             0.730111  \n",
       "5             0.930736  \n",
       "6             0.079365  \n",
       "7             0.536098  \n",
       "8             0.074661  \n",
       "9             0.989469  \n",
       "10            0.989469  \n",
       "11            0.930736  \n",
       "12            0.351707  \n",
       "13            0.994458  \n",
       "14            0.536098  \n",
       "15            0.944086  \n",
       "16            0.142857  \n",
       "17            1.000000  \n",
       "18            0.635485  \n",
       "19            0.660140  \n",
       "20            0.575175  \n",
       "21            0.997097  \n",
       "22            0.920516  \n",
       "23            0.980109  \n",
       "24            0.167821  \n",
       "25            1.000000  \n",
       "26            0.920516  \n",
       "27            0.832588  \n",
       "28            1.000000  \n",
       "29            0.479150  \n",
       "30            0.983137  \n",
       "31            0.990057  \n",
       "32            1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Extraire les dates min/max\n",
    "query_min_max = \"\"\"\n",
    "SELECT MIN(order_purchase_timestamp) AS min_date,\n",
    "       MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders\n",
    "\"\"\"\n",
    "min_date, max_date = pd.read_sql_query(query_min_max, conn).iloc[0]\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = pd.to_datetime(max_date)\n",
    "\n",
    "# 2. GÃ©nÃ©rer des bornes de pÃ©riodes tous les 15 jours\n",
    "quinzaines = pd.date_range(start=min_date, end=max_date, freq='15D')\n",
    "\n",
    "# 3. VÃ©rifier les pÃ©riodes avec commandes\n",
    "df_dates = pd.read_sql_query(\"SELECT order_purchase_timestamp FROM orders\", conn)\n",
    "df_dates['order_purchase_timestamp'] = pd.to_datetime(df_dates['order_purchase_timestamp'])\n",
    "\n",
    "# 4. GÃ©nÃ©rer le RFM pour chaque pÃ©riode de 15 jours\n",
    "rfm_all_periods = []\n",
    "\n",
    "for i in range(len(quinzaines) - 1):\n",
    "    start = quinzaines[i]\n",
    "    end = quinzaines[i + 1]\n",
    "\n",
    "    # VÃ©rifier prÃ©sence de commandes\n",
    "    has_orders = df_dates[\n",
    "        (df_dates['order_purchase_timestamp'] >= start) &\n",
    "        (df_dates['order_purchase_timestamp'] < end)\n",
    "    ]\n",
    "\n",
    "    if has_orders.empty:\n",
    "        continue\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT DATE('{end.date()}') AS global_max_date\n",
    "    ),\n",
    "    customer_orders AS (\n",
    "        SELECT \n",
    "            c.customer_unique_id,\n",
    "            o.order_id,\n",
    "            o.order_purchase_timestamp\n",
    "        FROM customers c\n",
    "        JOIN orders o ON c.customer_id = o.customer_id\n",
    "        WHERE o.order_purchase_timestamp BETWEEN '{start.date()}' AND '{end.date()}'\n",
    "    ),\n",
    "    payments AS (\n",
    "        SELECT \n",
    "            co.customer_unique_id,\n",
    "            MAX(co.order_purchase_timestamp) AS last_order_date,\n",
    "            COUNT(DISTINCT co.order_id) AS frequency,\n",
    "            SUM(op.payment_value) AS monetary\n",
    "        FROM customer_orders co\n",
    "        JOIN order_pymts op ON co.order_id = op.order_id\n",
    "        GROUP BY co.customer_unique_id\n",
    "    ),\n",
    "    rfm AS (\n",
    "        SELECT \n",
    "            p.customer_unique_id,\n",
    "            CAST(JULIANDAY(m.global_max_date) - JULIANDAY(p.last_order_date) AS INT) AS recency,\n",
    "            p.frequency,\n",
    "            ROUND(p.monetary, 2) AS monetary\n",
    "        FROM payments p\n",
    "        CROSS JOIN max_date m\n",
    "    )\n",
    "\n",
    "    SELECT *, '{start.date()} â†’ {end.date()}' AS period\n",
    "    FROM rfm\n",
    "    \"\"\"\n",
    "\n",
    "    df_rfm = pd.read_sql_query(query, conn)\n",
    "    rfm_all_periods.append(df_rfm)\n",
    "\n",
    "# Fusionner tous les RFM\n",
    "rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n",
    "\n",
    "# 5. Ã‰valuer la stabilitÃ© entre pÃ©riodes consÃ©cutives\n",
    "periods = sorted(rfm_all_periods['period'].unique())\n",
    "results = []\n",
    "\n",
    "for i in range(len(periods) - 1):\n",
    "    period_1 = periods[i]\n",
    "    period_2 = periods[i + 1]\n",
    "\n",
    "    df1 = rfm_all_periods[rfm_all_periods['period'] == period_1].copy()\n",
    "    df2 = rfm_all_periods[rfm_all_periods['period'] == period_2].copy()\n",
    "\n",
    "    common_ids = set(df1.customer_unique_id).intersection(df2.customer_unique_id)\n",
    "    df1 = df1[df1.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "    df2 = df2[df2.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "\n",
    "    if len(common_ids) < 5:\n",
    "        continue  # ignorer si trop peu de clients communs\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(df1[['recency', 'frequency', 'monetary']])\n",
    "    X2 = scaler.fit_transform(df2[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "    # Clustering\n",
    "    kmeans_1 = KMeans(n_clusters=4, random_state=42).fit(X1)\n",
    "    kmeans_2 = KMeans(n_clusters=4, random_state=42).fit(X2)\n",
    "\n",
    "    dbscan_1 = DBSCAN(eps=1.0, min_samples=5).fit(X1)\n",
    "    dbscan_2 = DBSCAN(eps=1.0, min_samples=5).fit(X2)\n",
    "\n",
    "    # StabilitÃ© des clusters\n",
    "    ari_kmeans = adjusted_rand_score(kmeans_1.labels_, kmeans_2.labels_)\n",
    "    ari_dbscan = adjusted_rand_score(dbscan_1.labels_, dbscan_2.labels_)\n",
    "\n",
    "    # Test KS pour la stabilitÃ© des distributions\n",
    "    ks_results = {\n",
    "        'recency': ks_2samp(df1['recency'], df2['recency']).pvalue,\n",
    "        'frequency': ks_2samp(df1['frequency'], df2['frequency']).pvalue,\n",
    "        'monetary': ks_2samp(df1['monetary'], df2['monetary']).pvalue,\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'period_pair': f\"{period_1} â†’ {period_2}\",\n",
    "        'n_clients_common': len(common_ids),\n",
    "        'ARI_kmeans': ari_kmeans,\n",
    "        'ARI_dbscan': ari_dbscan,\n",
    "        'KS_pvalue_recency': ks_results['recency'],\n",
    "        'KS_pvalue_frequency': ks_results['frequency'],\n",
    "        'KS_pvalue_monetary': ks_results['monetary'],\n",
    "    })\n",
    "\n",
    "# 6. RÃ©sultat final\n",
    "df_eval = pd.DataFrame(results)\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc79a92-5e6b-4dbd-a6eb-17d3afd8e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_10660\\1387553555.py:75: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_pair</th>\n",
       "      <th>n_clients_common</th>\n",
       "      <th>ARI_kmeans</th>\n",
       "      <th>ARI_dbscan</th>\n",
       "      <th>KS_pvalue_recency</th>\n",
       "      <th>KS_pvalue_frequency</th>\n",
       "      <th>KS_pvalue_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-23 â†’ 2017-11-28 â†’ 2017-11-28 â†’ 2017-12-03</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02 â†’ 2018-01-07 â†’ 2018-01-07 â†’ 2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-02 â†’ 2018-05-07 â†’ 2018-05-07 â†’ 2018-05-12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962704</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.575175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-07-26 â†’ 2018-07-31 â†’ 2018-07-31 â†’ 2018-08-05</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         period_pair  n_clients_common  \\\n",
       "0  2017-11-23 â†’ 2017-11-28 â†’ 2017-11-28 â†’ 2017-12-03                 6   \n",
       "1  2018-01-02 â†’ 2018-01-07 â†’ 2018-01-07 â†’ 2018-01-12                 5   \n",
       "2  2018-05-02 â†’ 2018-05-07 â†’ 2018-05-07 â†’ 2018-05-12                 7   \n",
       "3  2018-07-26 â†’ 2018-07-31 â†’ 2018-07-31 â†’ 2018-08-05                 5   \n",
       "\n",
       "   ARI_kmeans  ARI_dbscan  KS_pvalue_recency  KS_pvalue_frequency  \\\n",
       "0   -0.250000         1.0           1.000000             1.000000   \n",
       "1   -0.111111         1.0           0.873016             1.000000   \n",
       "2   -0.037037         1.0           0.962704             0.999961   \n",
       "3   -0.111111         1.0           0.873016             1.000000   \n",
       "\n",
       "   KS_pvalue_monetary  \n",
       "0            0.930736  \n",
       "1            0.873016  \n",
       "2            0.575175  \n",
       "3            0.873016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Extraire les dates min/max\n",
    "query_min_max = \"\"\"\n",
    "SELECT MIN(order_purchase_timestamp) AS min_date,\n",
    "       MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders\n",
    "\"\"\"\n",
    "min_date, max_date = pd.read_sql_query(query_min_max, conn).iloc[0]\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = pd.to_datetime(max_date)\n",
    "\n",
    "# 2. GÃ©nÃ©rer des bornes de pÃ©riodes tous les 15 jours\n",
    "quinzaines = pd.date_range(start=min_date, end=max_date, freq='5D')\n",
    "\n",
    "# 3. VÃ©rifier les pÃ©riodes avec commandes\n",
    "df_dates = pd.read_sql_query(\"SELECT order_purchase_timestamp FROM orders\", conn)\n",
    "df_dates['order_purchase_timestamp'] = pd.to_datetime(df_dates['order_purchase_timestamp'])\n",
    "\n",
    "# 4. GÃ©nÃ©rer le RFM pour chaque pÃ©riode de 15 jours\n",
    "rfm_all_periods = []\n",
    "\n",
    "for i in range(len(quinzaines) - 1):\n",
    "    start = quinzaines[i]\n",
    "    end = quinzaines[i + 1]\n",
    "\n",
    "    # VÃ©rifier prÃ©sence de commandes\n",
    "    has_orders = df_dates[\n",
    "        (df_dates['order_purchase_timestamp'] >= start) &\n",
    "        (df_dates['order_purchase_timestamp'] < end)\n",
    "    ]\n",
    "\n",
    "    if has_orders.empty:\n",
    "        continue\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT DATE('{end.date()}') AS global_max_date\n",
    "    ),\n",
    "    customer_orders AS (\n",
    "        SELECT \n",
    "            c.customer_unique_id,\n",
    "            o.order_id,\n",
    "            o.order_purchase_timestamp\n",
    "        FROM customers c\n",
    "        JOIN orders o ON c.customer_id = o.customer_id\n",
    "        WHERE o.order_purchase_timestamp BETWEEN '{start.date()}' AND '{end.date()}'\n",
    "    ),\n",
    "    payments AS (\n",
    "        SELECT \n",
    "            co.customer_unique_id,\n",
    "            MAX(co.order_purchase_timestamp) AS last_order_date,\n",
    "            COUNT(DISTINCT co.order_id) AS frequency,\n",
    "            SUM(op.payment_value) AS monetary\n",
    "        FROM customer_orders co\n",
    "        JOIN order_pymts op ON co.order_id = op.order_id\n",
    "        GROUP BY co.customer_unique_id\n",
    "    ),\n",
    "    rfm AS (\n",
    "        SELECT \n",
    "            p.customer_unique_id,\n",
    "            CAST(JULIANDAY(m.global_max_date) - JULIANDAY(p.last_order_date) AS INT) AS recency,\n",
    "            p.frequency,\n",
    "            ROUND(p.monetary, 2) AS monetary\n",
    "        FROM payments p\n",
    "        CROSS JOIN max_date m\n",
    "    )\n",
    "\n",
    "    SELECT *, '{start.date()} â†’ {end.date()}' AS period\n",
    "    FROM rfm\n",
    "    \"\"\"\n",
    "\n",
    "    df_rfm = pd.read_sql_query(query, conn)\n",
    "    rfm_all_periods.append(df_rfm)\n",
    "\n",
    "# Fusionner tous les RFM\n",
    "rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n",
    "\n",
    "# 5. Ã‰valuer la stabilitÃ© entre pÃ©riodes consÃ©cutives\n",
    "periods = sorted(rfm_all_periods['period'].unique())\n",
    "results = []\n",
    "\n",
    "for i in range(len(periods) - 1):\n",
    "    period_1 = periods[i]\n",
    "    period_2 = periods[i + 1]\n",
    "\n",
    "    df1 = rfm_all_periods[rfm_all_periods['period'] == period_1].copy()\n",
    "    df2 = rfm_all_periods[rfm_all_periods['period'] == period_2].copy()\n",
    "\n",
    "    common_ids = set(df1.customer_unique_id).intersection(df2.customer_unique_id)\n",
    "    df1 = df1[df1.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "    df2 = df2[df2.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "\n",
    "    if len(common_ids) < 5:\n",
    "        continue  # ignorer si trop peu de clients communs\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(df1[['recency', 'frequency', 'monetary']])\n",
    "    X2 = scaler.fit_transform(df2[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "    # Clustering\n",
    "    kmeans_1 = KMeans(n_clusters=4, random_state=42).fit(X1)\n",
    "    kmeans_2 = KMeans(n_clusters=4, random_state=42).fit(X2)\n",
    "\n",
    "    dbscan_1 = DBSCAN(eps=1.0, min_samples=5).fit(X1)\n",
    "    dbscan_2 = DBSCAN(eps=1.0, min_samples=5).fit(X2)\n",
    "\n",
    "    # StabilitÃ© des clusters\n",
    "    ari_kmeans = adjusted_rand_score(kmeans_1.labels_, kmeans_2.labels_)\n",
    "    ari_dbscan = adjusted_rand_score(dbscan_1.labels_, dbscan_2.labels_)\n",
    "\n",
    "    # Test KS pour la stabilitÃ© des distributions\n",
    "    ks_results = {\n",
    "        'recency': ks_2samp(df1['recency'], df2['recency']).pvalue,\n",
    "        'frequency': ks_2samp(df1['frequency'], df2['frequency']).pvalue,\n",
    "        'monetary': ks_2samp(df1['monetary'], df2['monetary']).pvalue,\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'period_pair': f\"{period_1} â†’ {period_2}\",\n",
    "        'n_clients_common': len(common_ids),\n",
    "        'ARI_kmeans': ari_kmeans,\n",
    "        'ARI_dbscan': ari_dbscan,\n",
    "        'KS_pvalue_recency': ks_results['recency'],\n",
    "        'KS_pvalue_frequency': ks_results['frequency'],\n",
    "        'KS_pvalue_monetary': ks_results['monetary'],\n",
    "    })\n",
    "\n",
    "# 6. RÃ©sultat final\n",
    "df_eval = pd.DataFrame(results)\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78d17c-17eb-4e06-a140-fe73345beef8",
   "metadata": {},
   "source": [
    "| ðŸ§  Conclusion Globale                                         | âœ… InterprÃ©tation                                                                                                                                  |\n",
    "|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| âœ”ï¸ Distributions RFM stables                                 | Les tests de Kolmogorov-Smirnov montrent que les variables `recency`, `frequency` et `monetary` ne varient pas significativement entre pÃ©riodes. |\n",
    "| âŒ Clusters (KMeans) instables                               | Les indices ARI sont faibles ou nÃ©gatifs â†’ faible cohÃ©rence des regroupements entre pÃ©riodes.                                                     |\n",
    "| âž¤ HypothÃ¨se 1 : Ã‰chantillon trop petit                      | Peu de clients communs (5 Ã  7) â†’ peu de donnÃ©es pour former des clusters fiables.                                                                 |\n",
    "| âž¤ HypothÃ¨se 2 : Comportement client trop changeant          | Les clients peuvent avoir des comportements trÃ¨s variables Ã  lâ€™Ã©chelle de 5 jours.                                                                |\n",
    "| âž¤ HypothÃ¨se 3 : Clustering inadaptÃ©                         | KMeans suppose des clusters sphÃ©riques et Ã©quilibrÃ©s, ce qui nâ€™est peut-Ãªtre pas le cas ici.                                                      |\n",
    "| âœ”ï¸ Clusters (DBSCAN) plus stables                            | DBSCAN montre une stabilitÃ© des regroupements (ARI = 1), ce qui indique une meilleure capacitÃ© Ã  identifier des structures sous-jacentes.         |\n",
    "| âž¤ HypothÃ¨se 4 : DBSCAN dÃ©tecte des structures non sphÃ©riques | Contrairement Ã  KMeans, DBSCAN peut mieux gÃ©rer des structures complexes (clusters de formes variÃ©es) sans faire d'hypothÃ¨ses sur leur forme.    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea726fe1-8efd-4a00-b4a2-572de6fb1a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e891adb-7715-42a4-bef5-51c0df115110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b62272-7a94-444d-b82f-40c5d365468d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Segmenter Des Clients (Poetry)",
   "language": "python",
   "name": "segmenter-des-clients-30gzothh-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
