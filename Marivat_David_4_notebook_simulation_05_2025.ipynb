{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bceac7d2-eb00-45d1-aabd-d63a42f4e16b",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold;\"><u>Import et chargement des données</u></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368acf0e-0aa4-403e-a345-72d08166993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.stats import ks_2samp\n",
    "from myFonctions import (\n",
    "    get_cluster_labels,\n",
    "    compute_ari_between_periods,\n",
    "    compute_ks_test_between_periods,\n",
    "    plot_ari,\n",
    "    plot_feature_distribution\n",
    ")\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ecfb785-a16e-485b-b407-18b24472d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers la base de données SQLite\n",
    "db_path = 'data/olist.db'\n",
    "\n",
    "# Créer une connexion à la base de données SQLite\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e970d743-4571-4317-a580-c48a8c4d3ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_pair</th>\n",
       "      <th>n_clients_common</th>\n",
       "      <th>ARI_kmeans</th>\n",
       "      <th>ARI_dbscan</th>\n",
       "      <th>KS_pvalue_recency</th>\n",
       "      <th>KS_pvalue_frequency</th>\n",
       "      <th>KS_pvalue_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017Q1 → 2017Q2</td>\n",
       "      <td>48</td>\n",
       "      <td>0.028896</td>\n",
       "      <td>0.226282</td>\n",
       "      <td>1.790296e-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017Q2 → 2017Q3</td>\n",
       "      <td>97</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.140347</td>\n",
       "      <td>7.071673e-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.963766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017Q3 → 2017Q4</td>\n",
       "      <td>132</td>\n",
       "      <td>0.018704</td>\n",
       "      <td>0.246018</td>\n",
       "      <td>4.940724e-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.969610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017Q4 → 2018Q1</td>\n",
       "      <td>137</td>\n",
       "      <td>0.028731</td>\n",
       "      <td>0.173639</td>\n",
       "      <td>1.088649e-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.860613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018Q1 → 2018Q2</td>\n",
       "      <td>195</td>\n",
       "      <td>0.017444</td>\n",
       "      <td>0.125570</td>\n",
       "      <td>3.598142e-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018Q2 → 2018Q3</td>\n",
       "      <td>102</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>6.693788e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       period_pair  n_clients_common  ARI_kmeans  ARI_dbscan  \\\n",
       "0  2017Q1 → 2017Q2                48    0.028896    0.226282   \n",
       "1  2017Q2 → 2017Q3                97    0.038491    0.140347   \n",
       "2  2017Q3 → 2017Q4               132    0.018704    0.246018   \n",
       "3  2017Q4 → 2018Q1               137    0.028731    0.173639   \n",
       "4  2018Q1 → 2018Q2               195    0.017444    0.125570   \n",
       "5  2018Q2 → 2018Q3               102    0.004690    0.038812   \n",
       "\n",
       "   KS_pvalue_recency  KS_pvalue_frequency  KS_pvalue_monetary  \n",
       "0       1.790296e-02                  1.0            0.692660  \n",
       "1       7.071673e-02                  1.0            0.963766  \n",
       "2       4.940724e-04                  1.0            0.969610  \n",
       "3       1.088649e-03                  1.0            0.860613  \n",
       "4       3.598142e-05                  1.0            0.081057  \n",
       "5       6.693788e-09                  1.0            0.994932  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Extraire les dates min/max\n",
    "query_min_max = \"\"\"\n",
    "SELECT MIN(order_purchase_timestamp) AS min_date,\n",
    "       MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders\n",
    "\"\"\"\n",
    "min_date, max_date = pd.read_sql_query(query_min_max, conn).iloc[0]\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = pd.to_datetime(max_date)\n",
    "\n",
    "# 2. Générer tous les trimestres\n",
    "trimestres = pd.period_range(start=min_date, end=max_date, freq='Q')\n",
    "\n",
    "# 3. Identifier les trimestres avec au moins une commande\n",
    "df_dates = pd.read_sql_query(\"SELECT order_purchase_timestamp FROM orders\", conn)\n",
    "df_dates['order_purchase_timestamp'] = pd.to_datetime(df_dates['order_purchase_timestamp'])\n",
    "df_dates['period'] = df_dates['order_purchase_timestamp'].dt.to_period('Q')\n",
    "trimestres_utiles = df_dates['period'].drop_duplicates().sort_values()\n",
    "\n",
    "# 4. Générer le RFM pour chaque trimestre\n",
    "rfm_all_periods = []\n",
    "\n",
    "for period in trimestres_utiles:\n",
    "    start = period.start_time\n",
    "    end = period.end_time\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT DATE('{end.date()}') AS global_max_date\n",
    "    ),\n",
    "    customer_orders AS (\n",
    "        SELECT \n",
    "            c.customer_unique_id,\n",
    "            o.order_id,\n",
    "            o.order_purchase_timestamp\n",
    "        FROM customers c\n",
    "        JOIN orders o ON c.customer_id = o.customer_id\n",
    "        WHERE o.order_purchase_timestamp BETWEEN '{start.date()}' AND '{end.date()}'\n",
    "    ),\n",
    "    payments AS (\n",
    "        SELECT \n",
    "            co.customer_unique_id,\n",
    "            MAX(co.order_purchase_timestamp) AS last_order_date,\n",
    "            COUNT(DISTINCT co.order_id) AS frequency,\n",
    "            SUM(op.payment_value) AS monetary\n",
    "        FROM customer_orders co\n",
    "        JOIN order_pymts op ON co.order_id = op.order_id\n",
    "        GROUP BY co.customer_unique_id\n",
    "    ),\n",
    "    rfm AS (\n",
    "        SELECT \n",
    "            p.customer_unique_id,\n",
    "            CAST(JULIANDAY(m.global_max_date) - JULIANDAY(p.last_order_date) AS INT) AS recency,\n",
    "            p.frequency,\n",
    "            ROUND(p.monetary, 2) AS monetary\n",
    "        FROM payments p\n",
    "        CROSS JOIN max_date m\n",
    "    )\n",
    "\n",
    "    SELECT *, '{period}' AS period\n",
    "    FROM rfm\n",
    "    \"\"\"\n",
    "\n",
    "    df_rfm = pd.read_sql_query(query, conn)\n",
    "    rfm_all_periods.append(df_rfm)\n",
    "\n",
    "rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n",
    "\n",
    "# 5. Calcul ARI et KS pour chaque paire de trimestres consécutifs\n",
    "periods = sorted(rfm_all_periods['period'].unique())\n",
    "results = []\n",
    "\n",
    "for i in range(len(periods) - 1):\n",
    "    period_1 = periods[i]\n",
    "    period_2 = periods[i + 1]\n",
    "\n",
    "    df1 = rfm_all_periods[rfm_all_periods['period'] == period_1].copy()\n",
    "    df2 = rfm_all_periods[rfm_all_periods['period'] == period_2].copy()\n",
    "\n",
    "    common_ids = set(df1.customer_unique_id).intersection(df2.customer_unique_id)\n",
    "    df1 = df1[df1.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "    df2 = df2[df2.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "\n",
    "    if len(common_ids) < 5:\n",
    "        continue  # ignorer si trop peu de clients communs\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(df1[['recency', 'frequency', 'monetary']])\n",
    "    X2 = scaler.fit_transform(df2[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "    kmeans_1 = KMeans(n_clusters=4, random_state=42).fit(X1)\n",
    "    kmeans_2 = KMeans(n_clusters=4, random_state=42).fit(X2)\n",
    "\n",
    "    dbscan_1 = DBSCAN(eps=1.0, min_samples=5).fit(X1)\n",
    "    dbscan_2 = DBSCAN(eps=1.0, min_samples=5).fit(X2)\n",
    "\n",
    "    ari_kmeans = adjusted_rand_score(kmeans_1.labels_, kmeans_2.labels_)\n",
    "    ari_dbscan = adjusted_rand_score(dbscan_1.labels_, dbscan_2.labels_)\n",
    "\n",
    "    ks_results = {\n",
    "        'recency': ks_2samp(df1['recency'], df2['recency']).pvalue,\n",
    "        'frequency': ks_2samp(df1['frequency'], df2['frequency']).pvalue,\n",
    "        'monetary': ks_2samp(df1['monetary'], df2['monetary']).pvalue,\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'period_pair': f\"{period_1} → {period_2}\",\n",
    "        'n_clients_common': len(common_ids),\n",
    "        'ARI_kmeans': ari_kmeans,\n",
    "        'ARI_dbscan': ari_dbscan,\n",
    "        'KS_pvalue_recency': ks_results['recency'],\n",
    "        'KS_pvalue_frequency': ks_results['frequency'],\n",
    "        'KS_pvalue_monetary': ks_results['monetary'],\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(results)\n",
    "display(df_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e578c-0fd5-4f7b-a661-946ab03b04d1",
   "metadata": {},
   "source": [
    "### 📊 Résumé des indicateurs d'évolution des clusters (RFM trimestriel)\n",
    "\n",
    "| Indicateur             | Résultat observé                  | Interprétation                                                                 |\n",
    "|------------------------|-----------------------------------|--------------------------------------------------------------------------------|\n",
    "| **ARI - KMeans**       | Très faible (max ≈ 0.04)          | Les clients changent fréquemment de groupe, **clustering instable**.          |\n",
    "| **ARI - DBSCAN**       | Faible à modéré (0.04 à 0.25)     | Un peu plus robuste que KMeans, mais **reste instable dans le temps**.        |\n",
    "| **KS Test – Recency**  | p-values souvent < 0.05           | **Variation significative** dans la récence : les clients changent d'activité. |\n",
    "| **KS Test – Frequency**| p-values ≈ 1.0                    | **Stable** : les clients achètent avec une fréquence similaire.               |\n",
    "| **KS Test – Monetary** | p-values élevées (souvent > 0.6)  | **Stable** : les montants dépensés restent globalement constants.             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab02d4-a1af-4b54-ab89-3d288aed8be2",
   "metadata": {},
   "source": [
    "📌 Analyse stratégique : passer à une analyse mensuelle ?\n",
    "Les résultats montrent que :\n",
    "\n",
    "Les comportements de fréquence et montant sont stables,\n",
    "\n",
    "Mais les clusters restent instables à cause de changements fréquents dans la récence, ce qui affecte fortement le RFM global.\n",
    "\n",
    "Cela soulève une question cruciale :\n",
    "👉 Est-ce que l’agrégation trimestrielle est trop grossière pour capturer les dynamiques comportementales des clients ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689278f-bacd-4178-9fc8-6b848670e82d",
   "metadata": {},
   "source": [
    "✅ Pourquoi tester le RFM tous les mois ?\n",
    "\n",
    "Argument\tAvantage attendu\n",
    "Plus grande granularité temporelle\tPermet de détecter plus tôt les changements d’habitude.\n",
    "Meilleure détection des événements récents\tUtile si le comportement client varie rapidement (ex : promotions, saisons).\n",
    "Peut révéler des micro-segments plus cohérents\tCertains segments peuvent être stables à court terme, mais invisibles à l’échelle trimestrielle.\n",
    "Suivi plus précis de la récence\tComme c’est la dimension la plus instable, elle sera mieux mesurée mensuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9792196-3c58-4fcc-a415-d86c96694b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_pair</th>\n",
       "      <th>n_clients_common</th>\n",
       "      <th>ARI_kmeans</th>\n",
       "      <th>ARI_dbscan</th>\n",
       "      <th>KS_pvalue_recency</th>\n",
       "      <th>KS_pvalue_frequency</th>\n",
       "      <th>KS_pvalue_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03 → 2017-04</td>\n",
       "      <td>13</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04 → 2017-05</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.086567</td>\n",
       "      <td>-0.136553</td>\n",
       "      <td>0.635485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05 → 2017-06</td>\n",
       "      <td>18</td>\n",
       "      <td>0.070729</td>\n",
       "      <td>0.061350</td>\n",
       "      <td>0.132394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-06 → 2017-07</td>\n",
       "      <td>15</td>\n",
       "      <td>0.021253</td>\n",
       "      <td>0.184994</td>\n",
       "      <td>0.678138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07 → 2017-08</td>\n",
       "      <td>23</td>\n",
       "      <td>0.076579</td>\n",
       "      <td>0.072168</td>\n",
       "      <td>0.024720</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.421782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-08 → 2017-09</td>\n",
       "      <td>31</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>-0.073979</td>\n",
       "      <td>0.079120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-09 → 2017-10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.089063</td>\n",
       "      <td>0.096886</td>\n",
       "      <td>0.392945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-10 → 2017-11</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.010073</td>\n",
       "      <td>-0.064352</td>\n",
       "      <td>0.706867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.510011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-11 → 2017-12</td>\n",
       "      <td>40</td>\n",
       "      <td>0.132505</td>\n",
       "      <td>0.153288</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-12 → 2018-01</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.084507</td>\n",
       "      <td>-0.118145</td>\n",
       "      <td>0.537929</td>\n",
       "      <td>0.978078</td>\n",
       "      <td>0.537929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01 → 2018-02</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.004552</td>\n",
       "      <td>-0.100806</td>\n",
       "      <td>0.935662</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.753697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-02 → 2018-03</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.070781</td>\n",
       "      <td>-0.009443</td>\n",
       "      <td>0.124295</td>\n",
       "      <td>0.992377</td>\n",
       "      <td>0.421782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-03 → 2018-04</td>\n",
       "      <td>34</td>\n",
       "      <td>0.316368</td>\n",
       "      <td>0.233816</td>\n",
       "      <td>0.672684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-04 → 2018-05</td>\n",
       "      <td>44</td>\n",
       "      <td>0.083338</td>\n",
       "      <td>-0.053163</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.318829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-05 → 2018-06</td>\n",
       "      <td>38</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>-0.061147</td>\n",
       "      <td>0.737879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-06 → 2018-07</td>\n",
       "      <td>21</td>\n",
       "      <td>0.179783</td>\n",
       "      <td>0.101604</td>\n",
       "      <td>0.602813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-07 → 2018-08</td>\n",
       "      <td>48</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>0.497903</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.960172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-08 → 2018-09</td>\n",
       "      <td>8</td>\n",
       "      <td>0.269565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "      <td>0.980109</td>\n",
       "      <td>0.980109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          period_pair  n_clients_common  ARI_kmeans  ARI_dbscan  \\\n",
       "0   2017-03 → 2017-04                13    0.014440    0.082353   \n",
       "1   2017-04 → 2017-05                14   -0.086567   -0.136553   \n",
       "2   2017-05 → 2017-06                18    0.070729    0.061350   \n",
       "3   2017-06 → 2017-07                15    0.021253    0.184994   \n",
       "4   2017-07 → 2017-08                23    0.076579    0.072168   \n",
       "5   2017-08 → 2017-09                31    0.048171   -0.073979   \n",
       "6   2017-09 → 2017-10                30    0.089063    0.096886   \n",
       "7   2017-10 → 2017-11                36   -0.010073   -0.064352   \n",
       "8   2017-11 → 2017-12                40    0.132505    0.153288   \n",
       "9   2017-12 → 2018-01                19   -0.084507   -0.118145   \n",
       "10  2018-01 → 2018-02                27   -0.004552   -0.100806   \n",
       "11  2018-02 → 2018-03                23   -0.070781   -0.009443   \n",
       "12  2018-03 → 2018-04                34    0.316368    0.233816   \n",
       "13  2018-04 → 2018-05                44    0.083338   -0.053163   \n",
       "14  2018-05 → 2018-06                38    0.008463   -0.061147   \n",
       "15  2018-06 → 2018-07                21    0.179783    0.101604   \n",
       "16  2018-07 → 2018-08                48    0.034450    0.497903   \n",
       "17  2018-08 → 2018-09                 8    0.269565    1.000000   \n",
       "\n",
       "    KS_pvalue_recency  KS_pvalue_frequency  KS_pvalue_monetary  \n",
       "0            0.126488             1.000000            0.999212  \n",
       "1            0.635485             1.000000            0.999592  \n",
       "2            0.132394             1.000000            0.971540  \n",
       "3            0.678138             1.000000            0.938331  \n",
       "4            0.024720             0.999999            0.421782  \n",
       "5            0.079120             1.000000            0.823454  \n",
       "6            0.392945             1.000000            0.807963  \n",
       "7            0.706867             1.000000            0.510011  \n",
       "8            0.000066             1.000000            0.918805  \n",
       "9            0.537929             0.978078            0.537929  \n",
       "10           0.935662             1.000000            0.753697  \n",
       "11           0.124295             0.992377            0.421782  \n",
       "12           0.672684             1.000000            0.672684  \n",
       "13           0.011526             1.000000            0.318829  \n",
       "14           0.737879             1.000000            0.903253  \n",
       "15           0.602813             1.000000            0.987044  \n",
       "16           0.000956             0.999992            0.960172  \n",
       "17           0.660140             0.980109            0.980109  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"ks_2samp: Exact calculation unsuccessful.*\")\n",
    "# 1. Extraire les dates min/max\n",
    "query_min_max = \"\"\"\n",
    "SELECT MIN(order_purchase_timestamp) AS min_date,\n",
    "       MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders\n",
    "\"\"\"\n",
    "min_date, max_date = pd.read_sql_query(query_min_max, conn).iloc[0]\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = pd.to_datetime(max_date)\n",
    "\n",
    "# 2. Générer tous les mois\n",
    "mois = pd.period_range(start=min_date, end=max_date, freq='M')\n",
    "\n",
    "# 3. Identifier les mois avec au moins une commande\n",
    "df_dates = pd.read_sql_query(\"SELECT order_purchase_timestamp FROM orders\", conn)\n",
    "df_dates['order_purchase_timestamp'] = pd.to_datetime(df_dates['order_purchase_timestamp'])\n",
    "df_dates['period'] = df_dates['order_purchase_timestamp'].dt.to_period('M')\n",
    "mois_utiles = df_dates['period'].drop_duplicates().sort_values()\n",
    "\n",
    "# 4. Générer le RFM pour chaque mois\n",
    "rfm_all_periods = []\n",
    "\n",
    "for period in mois_utiles:\n",
    "    start = period.start_time\n",
    "    end = period.end_time\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT DATE('{end.date()}') AS global_max_date\n",
    "    ),\n",
    "    customer_orders AS (\n",
    "        SELECT \n",
    "            c.customer_unique_id,\n",
    "            o.order_id,\n",
    "            o.order_purchase_timestamp\n",
    "        FROM customers c\n",
    "        JOIN orders o ON c.customer_id = o.customer_id\n",
    "        WHERE o.order_purchase_timestamp BETWEEN '{start.date()}' AND '{end.date()}'\n",
    "    ),\n",
    "    payments AS (\n",
    "        SELECT \n",
    "            co.customer_unique_id,\n",
    "            MAX(co.order_purchase_timestamp) AS last_order_date,\n",
    "            COUNT(DISTINCT co.order_id) AS frequency,\n",
    "            SUM(op.payment_value) AS monetary\n",
    "        FROM customer_orders co\n",
    "        JOIN order_pymts op ON co.order_id = op.order_id\n",
    "        GROUP BY co.customer_unique_id\n",
    "    ),\n",
    "    rfm AS (\n",
    "        SELECT \n",
    "            p.customer_unique_id,\n",
    "            CAST(JULIANDAY(m.global_max_date) - JULIANDAY(p.last_order_date) AS INT) AS recency,\n",
    "            p.frequency,\n",
    "            ROUND(p.monetary, 2) AS monetary\n",
    "        FROM payments p\n",
    "        CROSS JOIN max_date m\n",
    "    )\n",
    "\n",
    "    SELECT *, '{period}' AS period\n",
    "    FROM rfm\n",
    "    \"\"\"\n",
    "\n",
    "    df_rfm = pd.read_sql_query(query, conn)\n",
    "    rfm_all_periods.append(df_rfm)\n",
    "\n",
    "rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n",
    "\n",
    "# 5. Calcul ARI et KS pour chaque paire de mois consécutifs\n",
    "periods = sorted(rfm_all_periods['period'].unique())\n",
    "results = []\n",
    "\n",
    "for i in range(len(periods) - 1):\n",
    "    period_1 = periods[i]\n",
    "    period_2 = periods[i + 1]\n",
    "\n",
    "    df1 = rfm_all_periods[rfm_all_periods['period'] == period_1].copy()\n",
    "    df2 = rfm_all_periods[rfm_all_periods['period'] == period_2].copy()\n",
    "\n",
    "    common_ids = set(df1.customer_unique_id).intersection(df2.customer_unique_id)\n",
    "    df1 = df1[df1.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "    df2 = df2[df2.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "\n",
    "    if len(common_ids) < 5:\n",
    "        continue  # ignorer si trop peu de clients communs\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(df1[['recency', 'frequency', 'monetary']])\n",
    "    X2 = scaler.fit_transform(df2[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "    kmeans_1 = KMeans(n_clusters=4, random_state=42).fit(X1)\n",
    "    kmeans_2 = KMeans(n_clusters=4, random_state=42).fit(X2)\n",
    "\n",
    "    dbscan_1 = DBSCAN(eps=1.0, min_samples=5).fit(X1)\n",
    "    dbscan_2 = DBSCAN(eps=1.0, min_samples=5).fit(X2)\n",
    "\n",
    "    ari_kmeans = adjusted_rand_score(kmeans_1.labels_, kmeans_2.labels_)\n",
    "    ari_dbscan = adjusted_rand_score(dbscan_1.labels_, dbscan_2.labels_)\n",
    "\n",
    "    ks_results = {\n",
    "        'recency': ks_2samp(df1['recency'], df2['recency']).pvalue,\n",
    "        'frequency': ks_2samp(df1['frequency'], df2['frequency']).pvalue,\n",
    "        'monetary': ks_2samp(df1['monetary'], df2['monetary']).pvalue,\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'period_pair': f\"{period_1} → {period_2}\",\n",
    "        'n_clients_common': len(common_ids),\n",
    "        'ARI_kmeans': ari_kmeans,\n",
    "        'ARI_dbscan': ari_dbscan,\n",
    "        'KS_pvalue_recency': ks_results['recency'],\n",
    "        'KS_pvalue_frequency': ks_results['frequency'],\n",
    "        'KS_pvalue_monetary': ks_results['monetary'],\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(results)\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ebac8-a262-463a-9d47-b4d263958768",
   "metadata": {},
   "source": [
    "| Période               | Clients Communs | ARI KMeans | ARI DBSCAN | RFM Change ?      | KMeans Stable ? | DBSCAN Stable ? | Conclusion Synthétique                                       |\n",
    "|-----------------------|------------------|------------|------------|-------------------|------------------|------------------|----------------------------------------------------------------|\n",
    "| 2017-03 → 2017-04     | 13               | 0.014      | 0.082      | 🟢 Stable         | ❌ Faible        | ❌ Faible        | Clusters très instables, pas de changement RFM                |\n",
    "| 2017-04 → 2017-05     | 14               | -0.087     | -0.137     | 🟢 Stable         | ❌ Très faible   | ❌ Très faible   | Instabilité des deux modèles sans évolution comportementale    |\n",
    "| 2017-05 → 2017-06     | 18               | 0.071      | 0.061      | 🟢 Stable         | ⚠️ Moyenne       | ⚠️ Moyenne       | Légère stabilité, comportement clients constant                |\n",
    "| 2017-06 → 2017-07     | 15               | 0.021      | 0.185      | 🟢 Stable         | ❌ Faible        | ⚠️ Moyenne       | DBSCAN un peu plus cohérent que KMeans                        |\n",
    "| 2017-07 → 2017-08     | 23               | 0.077      | 0.072      | 🔥 Recency        | ⚠️ Moyenne       | ⚠️ Moyenne       | Début de changement RFM, légers ajustements de cluster        |\n",
    "| 2017-08 → 2017-09     | 31               | 0.048      | -0.074     | 🟢 Stable         | ⚠️ Faible        | ❌ Très faible   | Faible cohérence, surtout DBSCAN                             |\n",
    "| 2017-09 → 2017-10     | 30               | 0.089      | 0.097      | 🟢 Stable         | ⚠️ Moyenne       | ⚠️ Moyenne       | Stabilité modérée dans les deux approches                     |\n",
    "| 2017-10 → 2017-11     | 36               | -0.010     | -0.064     | 🟢 Stable         | ❌ Très faible   | ❌ Très faible   | Instabilité injustifiée dans les deux modèles                 |\n",
    "| 2017-11 → 2017-12     | 40               | 0.133      | 0.153      | 🔥 Recency        | ⚠️ Moyenne       | ⚠️ Moyenne       | Récence change, stabilité moyenne                            |\n",
    "| 2017-12 → 2018-01     | 19               | -0.085     | -0.118     | 🟢 Stable         | ❌ Faible        | ❌ Faible        | Cluster incohérent sans changement de fond                    |\n",
    "| 2018-01 → 2018-02     | 27               | -0.005     | -0.101     | 🟢 Stable         | ❌ Faible        | ❌ Faible        | Faible stabilité malgré comportement stable                   |\n",
    "| 2018-02 → 2018-03     | 23               | -0.071     | -0.009     | 🟢 Stable         | ❌ Faible        | ⚠️ Moyenne       | Léger mieux pour DBSCAN, mais pas convaincant                 |\n",
    "| 2018-03 → 2018-04     | 34               | 0.316      | 0.234      | 🟢 Stable         | ✅ Bonne         | ✅ Bonne         | Excellente cohérence dans les deux modèles                    |\n",
    "| 2018-04 → 2018-05     | 44               | 0.083      | -0.053     | 🔥 Recency        | ⚠️ Moyenne       | ❌ Faible        | DBSCAN perd en cohérence malgré rupture récence              |\n",
    "| 2018-05 → 2018-06     | 38               | 0.008      | -0.061     | 🟢 Stable         | ❌ Faible        | ❌ Faible        | Incohérence persistante, pas de comportement nouveau          |\n",
    "| 2018-06 → 2018-07     | 21               | 0.180      | 0.102      | 🟢 Stable         | ⚠️ Moyenne       | ⚠️ Moyenne       | Meilleure tenue des clusters, stabilité comportementale       |\n",
    "| 2018-07 → 2018-08     | 48               | 0.034      | 0.498      | 🔥 Recency        | ❌ Faible        | ✅ Bonne         | DBSCAN réagit mieux à la rupture comportementale              |\n",
    "| 2018-08 → 2018-09     | 8                | 0.270      | 1.000      | 🟢 Stable         | ✅ Bonne         | ✅ Excellente     | Très forte stabilité, mais effectif trop faible pour conclure |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa0fb1d-e440-44f4-b899-12530d26aaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_pair</th>\n",
       "      <th>n_clients_common</th>\n",
       "      <th>ARI_kmeans</th>\n",
       "      <th>ARI_dbscan</th>\n",
       "      <th>KS_pvalue_recency</th>\n",
       "      <th>KS_pvalue_frequency</th>\n",
       "      <th>KS_pvalue_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-01 → 2017-02-16 → 2017-02-16 → 2017-03-03</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-03-18 → 2017-04-02 → 2017-04-02 → 2017-04-17</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-02 → 2017-04-17 → 2017-04-17 → 2017-05-02</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-17 → 2017-05-02 → 2017-05-02 → 2017-05-17</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575175</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.962704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-02 → 2017-05-17 → 2017-05-17 → 2017-06-01</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.020243</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.125874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-05-17 → 2017-06-01 → 2017-06-01 → 2017-06-16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-06-01 → 2017-06-16 → 2017-06-16 → 2017-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-07-01 → 2017-07-16 → 2017-07-16 → 2017-07-31</td>\n",
       "      <td>12</td>\n",
       "      <td>0.030651</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.099547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-07-16 → 2017-07-31 → 2017-07-31 → 2017-08-15</td>\n",
       "      <td>11</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-07-31 → 2017-08-15 → 2017-08-15 → 2017-08-30</td>\n",
       "      <td>9</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-08-30 → 2017-09-14 → 2017-09-14 → 2017-09-29</td>\n",
       "      <td>9</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-09-14 → 2017-09-29 → 2017-09-29 → 2017-10-14</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.153846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-09-29 → 2017-10-14 → 2017-10-14 → 2017-10-29</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.351707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-10-14 → 2017-10-29 → 2017-10-29 → 2017-11-13</td>\n",
       "      <td>10</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-10-29 → 2017-11-13 → 2017-11-13 → 2017-11-28</td>\n",
       "      <td>12</td>\n",
       "      <td>0.102285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-11-13 → 2017-11-28 → 2017-11-28 → 2017-12-13</td>\n",
       "      <td>28</td>\n",
       "      <td>0.136773</td>\n",
       "      <td>0.193483</td>\n",
       "      <td>0.010890</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-11-28 → 2017-12-13 → 2017-12-13 → 2017-12-28</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-12-13 → 2017-12-28 → 2017-12-28 → 2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-12-28 → 2018-01-12 → 2018-01-12 → 2018-01-27</td>\n",
       "      <td>14</td>\n",
       "      <td>0.151399</td>\n",
       "      <td>-0.060909</td>\n",
       "      <td>0.343320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.635485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-12 → 2018-01-27 → 2018-01-27 → 2018-02-11</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-02-11 → 2018-02-26 → 2018-02-26 → 2018-03-13</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.235294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.575175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-02-26 → 2018-03-13 → 2018-03-13 → 2018-03-28</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.116505</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-03-13 → 2018-03-28 → 2018-03-28 → 2018-04-12</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.007179</td>\n",
       "      <td>-0.060909</td>\n",
       "      <td>0.920516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-03-28 → 2018-04-12 → 2018-04-12 → 2018-04-27</td>\n",
       "      <td>8</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-04-12 → 2018-04-27 → 2018-04-27 → 2018-05-12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-04-27 → 2018-05-12 → 2018-05-12 → 2018-05-27</td>\n",
       "      <td>12</td>\n",
       "      <td>0.398714</td>\n",
       "      <td>-0.079019</td>\n",
       "      <td>0.536098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-05-12 → 2018-05-27 → 2018-05-27 → 2018-06-11</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.011111</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.343320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-05-27 → 2018-06-11 → 2018-06-11 → 2018-06-26</td>\n",
       "      <td>11</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>-0.026667</td>\n",
       "      <td>0.832588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-06-11 → 2018-06-26 → 2018-06-26 → 2018-07-11</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-06-26 → 2018-07-11 → 2018-07-11 → 2018-07-26</td>\n",
       "      <td>11</td>\n",
       "      <td>0.048443</td>\n",
       "      <td>-0.078431</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-07-11 → 2018-07-26 → 2018-07-26 → 2018-08-10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>0.602510</td>\n",
       "      <td>0.571336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-07-26 → 2018-08-10 → 2018-08-10 → 2018-08-25</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.030295</td>\n",
       "      <td>0.286689</td>\n",
       "      <td>0.632386</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.990057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-08-10 → 2018-08-25 → 2018-08-25 → 2018-09-09</td>\n",
       "      <td>9</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          period_pair  n_clients_common  \\\n",
       "0   2017-02-01 → 2017-02-16 → 2017-02-16 → 2017-03-03                 8   \n",
       "1   2017-03-18 → 2017-04-02 → 2017-04-02 → 2017-04-17                 5   \n",
       "2   2017-04-02 → 2017-04-17 → 2017-04-17 → 2017-05-02                 5   \n",
       "3   2017-04-17 → 2017-05-02 → 2017-05-02 → 2017-05-17                 7   \n",
       "4   2017-05-02 → 2017-05-17 → 2017-05-17 → 2017-06-01                 9   \n",
       "5   2017-05-17 → 2017-06-01 → 2017-06-01 → 2017-06-16                 6   \n",
       "6   2017-06-01 → 2017-06-16 → 2017-06-16 → 2017-07-01                 5   \n",
       "7   2017-07-01 → 2017-07-16 → 2017-07-16 → 2017-07-31                12   \n",
       "8   2017-07-16 → 2017-07-31 → 2017-07-31 → 2017-08-15                11   \n",
       "9   2017-07-31 → 2017-08-15 → 2017-08-15 → 2017-08-30                 9   \n",
       "10  2017-08-30 → 2017-09-14 → 2017-09-14 → 2017-09-29                 9   \n",
       "11  2017-09-14 → 2017-09-29 → 2017-09-29 → 2017-10-14                 6   \n",
       "12  2017-09-29 → 2017-10-14 → 2017-10-14 → 2017-10-29                 9   \n",
       "13  2017-10-14 → 2017-10-29 → 2017-10-29 → 2017-11-13                10   \n",
       "14  2017-10-29 → 2017-11-13 → 2017-11-13 → 2017-11-28                12   \n",
       "15  2017-11-13 → 2017-11-28 → 2017-11-28 → 2017-12-13                28   \n",
       "16  2017-11-28 → 2017-12-13 → 2017-12-13 → 2017-12-28                 6   \n",
       "17  2017-12-13 → 2017-12-28 → 2017-12-28 → 2018-01-12                 5   \n",
       "18  2017-12-28 → 2018-01-12 → 2018-01-12 → 2018-01-27                14   \n",
       "19  2018-01-12 → 2018-01-27 → 2018-01-27 → 2018-02-11                 8   \n",
       "20  2018-02-11 → 2018-02-26 → 2018-02-26 → 2018-03-13                 7   \n",
       "21  2018-02-26 → 2018-03-13 → 2018-03-13 → 2018-03-28                11   \n",
       "22  2018-03-13 → 2018-03-28 → 2018-03-28 → 2018-04-12                14   \n",
       "23  2018-03-28 → 2018-04-12 → 2018-04-12 → 2018-04-27                 8   \n",
       "24  2018-04-12 → 2018-04-27 → 2018-04-27 → 2018-05-12                10   \n",
       "25  2018-04-27 → 2018-05-12 → 2018-05-12 → 2018-05-27                12   \n",
       "26  2018-05-12 → 2018-05-27 → 2018-05-27 → 2018-06-11                14   \n",
       "27  2018-05-27 → 2018-06-11 → 2018-06-11 → 2018-06-26                11   \n",
       "28  2018-06-11 → 2018-06-26 → 2018-06-26 → 2018-07-11                 5   \n",
       "29  2018-06-26 → 2018-07-11 → 2018-07-11 → 2018-07-26                11   \n",
       "30  2018-07-11 → 2018-07-26 → 2018-07-26 → 2018-08-10                20   \n",
       "31  2018-07-26 → 2018-08-10 → 2018-08-10 → 2018-08-25                22   \n",
       "32  2018-08-10 → 2018-08-25 → 2018-08-25 → 2018-09-09                 9   \n",
       "\n",
       "    ARI_kmeans  ARI_dbscan  KS_pvalue_recency  KS_pvalue_frequency  \\\n",
       "0     0.026087    1.000000           0.660140             1.000000   \n",
       "1    -0.111111    1.000000           1.000000             1.000000   \n",
       "2    -0.111111    1.000000           0.357143             1.000000   \n",
       "3    -0.037037    1.000000           0.575175             0.999961   \n",
       "4    -0.020243   -0.012500           0.125874             1.000000   \n",
       "5     0.285714    1.000000           0.474026             1.000000   \n",
       "6    -0.111111    1.000000           1.000000             1.000000   \n",
       "7     0.030651    0.022222           0.099547             1.000000   \n",
       "8     0.036558    0.000000           0.479150             1.000000   \n",
       "9     0.172414    0.000000           0.351707             1.000000   \n",
       "10    0.428571    0.555556           0.006294             1.000000   \n",
       "11   -0.153846    1.000000           0.142857             1.000000   \n",
       "12   -0.185185    0.000000           0.989469             1.000000   \n",
       "13    0.123077    0.000000           0.417524             1.000000   \n",
       "14    0.102285    0.000000           0.868982             1.000000   \n",
       "15    0.136773    0.193483           0.010890             1.000000   \n",
       "16   -0.250000    1.000000           0.474026             1.000000   \n",
       "17   -0.111111    1.000000           0.873016             1.000000   \n",
       "18    0.151399   -0.060909           0.343320             1.000000   \n",
       "19   -0.083333    1.000000           0.660140             1.000000   \n",
       "20   -0.235294    1.000000           0.212121             0.999961   \n",
       "21   -0.116505   -0.100000           0.479150             1.000000   \n",
       "22   -0.007179   -0.060909           0.920516             1.000000   \n",
       "23    0.151515    1.000000           0.660140             1.000000   \n",
       "24    0.166667    0.000000           0.786930             1.000000   \n",
       "25    0.398714   -0.079019           0.536098             1.000000   \n",
       "26   -0.011111    0.048338           0.343320             1.000000   \n",
       "27    0.226804   -0.026667           0.832588             1.000000   \n",
       "28   -0.111111    1.000000           0.357143             1.000000   \n",
       "29    0.048443   -0.078431           0.479150             1.000000   \n",
       "30    0.009324    0.602510           0.571336             1.000000   \n",
       "31   -0.030295    0.286689           0.632386             0.999998   \n",
       "32    0.520000    1.000000           0.730111             1.000000   \n",
       "\n",
       "    KS_pvalue_monetary  \n",
       "0             0.660140  \n",
       "1             1.000000  \n",
       "2             1.000000  \n",
       "3             0.962704  \n",
       "4             0.730111  \n",
       "5             0.930736  \n",
       "6             0.079365  \n",
       "7             0.536098  \n",
       "8             0.074661  \n",
       "9             0.989469  \n",
       "10            0.989469  \n",
       "11            0.930736  \n",
       "12            0.351707  \n",
       "13            0.994458  \n",
       "14            0.536098  \n",
       "15            0.944086  \n",
       "16            0.142857  \n",
       "17            1.000000  \n",
       "18            0.635485  \n",
       "19            0.660140  \n",
       "20            0.575175  \n",
       "21            0.997097  \n",
       "22            0.920516  \n",
       "23            0.980109  \n",
       "24            0.167821  \n",
       "25            1.000000  \n",
       "26            0.920516  \n",
       "27            0.832588  \n",
       "28            1.000000  \n",
       "29            0.479150  \n",
       "30            0.983137  \n",
       "31            0.990057  \n",
       "32            1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Extraire les dates min/max\n",
    "query_min_max = \"\"\"\n",
    "SELECT MIN(order_purchase_timestamp) AS min_date,\n",
    "       MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders\n",
    "\"\"\"\n",
    "min_date, max_date = pd.read_sql_query(query_min_max, conn).iloc[0]\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = pd.to_datetime(max_date)\n",
    "\n",
    "# 2. Générer des bornes de périodes tous les 15 jours\n",
    "quinzaines = pd.date_range(start=min_date, end=max_date, freq='15D')\n",
    "\n",
    "# 3. Vérifier les périodes avec commandes\n",
    "df_dates = pd.read_sql_query(\"SELECT order_purchase_timestamp FROM orders\", conn)\n",
    "df_dates['order_purchase_timestamp'] = pd.to_datetime(df_dates['order_purchase_timestamp'])\n",
    "\n",
    "# 4. Générer le RFM pour chaque période de 15 jours\n",
    "rfm_all_periods = []\n",
    "\n",
    "for i in range(len(quinzaines) - 1):\n",
    "    start = quinzaines[i]\n",
    "    end = quinzaines[i + 1]\n",
    "\n",
    "    # Vérifier présence de commandes\n",
    "    has_orders = df_dates[\n",
    "        (df_dates['order_purchase_timestamp'] >= start) &\n",
    "        (df_dates['order_purchase_timestamp'] < end)\n",
    "    ]\n",
    "\n",
    "    if has_orders.empty:\n",
    "        continue\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT DATE('{end.date()}') AS global_max_date\n",
    "    ),\n",
    "    customer_orders AS (\n",
    "        SELECT \n",
    "            c.customer_unique_id,\n",
    "            o.order_id,\n",
    "            o.order_purchase_timestamp\n",
    "        FROM customers c\n",
    "        JOIN orders o ON c.customer_id = o.customer_id\n",
    "        WHERE o.order_purchase_timestamp BETWEEN '{start.date()}' AND '{end.date()}'\n",
    "    ),\n",
    "    payments AS (\n",
    "        SELECT \n",
    "            co.customer_unique_id,\n",
    "            MAX(co.order_purchase_timestamp) AS last_order_date,\n",
    "            COUNT(DISTINCT co.order_id) AS frequency,\n",
    "            SUM(op.payment_value) AS monetary\n",
    "        FROM customer_orders co\n",
    "        JOIN order_pymts op ON co.order_id = op.order_id\n",
    "        GROUP BY co.customer_unique_id\n",
    "    ),\n",
    "    rfm AS (\n",
    "        SELECT \n",
    "            p.customer_unique_id,\n",
    "            CAST(JULIANDAY(m.global_max_date) - JULIANDAY(p.last_order_date) AS INT) AS recency,\n",
    "            p.frequency,\n",
    "            ROUND(p.monetary, 2) AS monetary\n",
    "        FROM payments p\n",
    "        CROSS JOIN max_date m\n",
    "    )\n",
    "\n",
    "    SELECT *, '{start.date()} → {end.date()}' AS period\n",
    "    FROM rfm\n",
    "    \"\"\"\n",
    "\n",
    "    df_rfm = pd.read_sql_query(query, conn)\n",
    "    rfm_all_periods.append(df_rfm)\n",
    "\n",
    "# Fusionner tous les RFM\n",
    "rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n",
    "\n",
    "# 5. Évaluer la stabilité entre périodes consécutives\n",
    "periods = sorted(rfm_all_periods['period'].unique())\n",
    "results = []\n",
    "\n",
    "for i in range(len(periods) - 1):\n",
    "    period_1 = periods[i]\n",
    "    period_2 = periods[i + 1]\n",
    "\n",
    "    df1 = rfm_all_periods[rfm_all_periods['period'] == period_1].copy()\n",
    "    df2 = rfm_all_periods[rfm_all_periods['period'] == period_2].copy()\n",
    "\n",
    "    common_ids = set(df1.customer_unique_id).intersection(df2.customer_unique_id)\n",
    "    df1 = df1[df1.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "    df2 = df2[df2.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "\n",
    "    if len(common_ids) < 5:\n",
    "        continue  # ignorer si trop peu de clients communs\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(df1[['recency', 'frequency', 'monetary']])\n",
    "    X2 = scaler.fit_transform(df2[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "    # Clustering\n",
    "    kmeans_1 = KMeans(n_clusters=4, random_state=42).fit(X1)\n",
    "    kmeans_2 = KMeans(n_clusters=4, random_state=42).fit(X2)\n",
    "\n",
    "    dbscan_1 = DBSCAN(eps=1.0, min_samples=5).fit(X1)\n",
    "    dbscan_2 = DBSCAN(eps=1.0, min_samples=5).fit(X2)\n",
    "\n",
    "    # Stabilité des clusters\n",
    "    ari_kmeans = adjusted_rand_score(kmeans_1.labels_, kmeans_2.labels_)\n",
    "    ari_dbscan = adjusted_rand_score(dbscan_1.labels_, dbscan_2.labels_)\n",
    "\n",
    "    # Test KS pour la stabilité des distributions\n",
    "    ks_results = {\n",
    "        'recency': ks_2samp(df1['recency'], df2['recency']).pvalue,\n",
    "        'frequency': ks_2samp(df1['frequency'], df2['frequency']).pvalue,\n",
    "        'monetary': ks_2samp(df1['monetary'], df2['monetary']).pvalue,\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'period_pair': f\"{period_1} → {period_2}\",\n",
    "        'n_clients_common': len(common_ids),\n",
    "        'ARI_kmeans': ari_kmeans,\n",
    "        'ARI_dbscan': ari_dbscan,\n",
    "        'KS_pvalue_recency': ks_results['recency'],\n",
    "        'KS_pvalue_frequency': ks_results['frequency'],\n",
    "        'KS_pvalue_monetary': ks_results['monetary'],\n",
    "    })\n",
    "\n",
    "# 6. Résultat final\n",
    "df_eval = pd.DataFrame(results)\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc79a92-5e6b-4dbd-a6eb-17d3afd8e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_10660\\1387553555.py:75: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_pair</th>\n",
       "      <th>n_clients_common</th>\n",
       "      <th>ARI_kmeans</th>\n",
       "      <th>ARI_dbscan</th>\n",
       "      <th>KS_pvalue_recency</th>\n",
       "      <th>KS_pvalue_frequency</th>\n",
       "      <th>KS_pvalue_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-23 → 2017-11-28 → 2017-11-28 → 2017-12-03</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02 → 2018-01-07 → 2018-01-07 → 2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-02 → 2018-05-07 → 2018-05-07 → 2018-05-12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962704</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.575175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-07-26 → 2018-07-31 → 2018-07-31 → 2018-08-05</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         period_pair  n_clients_common  \\\n",
       "0  2017-11-23 → 2017-11-28 → 2017-11-28 → 2017-12-03                 6   \n",
       "1  2018-01-02 → 2018-01-07 → 2018-01-07 → 2018-01-12                 5   \n",
       "2  2018-05-02 → 2018-05-07 → 2018-05-07 → 2018-05-12                 7   \n",
       "3  2018-07-26 → 2018-07-31 → 2018-07-31 → 2018-08-05                 5   \n",
       "\n",
       "   ARI_kmeans  ARI_dbscan  KS_pvalue_recency  KS_pvalue_frequency  \\\n",
       "0   -0.250000         1.0           1.000000             1.000000   \n",
       "1   -0.111111         1.0           0.873016             1.000000   \n",
       "2   -0.037037         1.0           0.962704             0.999961   \n",
       "3   -0.111111         1.0           0.873016             1.000000   \n",
       "\n",
       "   KS_pvalue_monetary  \n",
       "0            0.930736  \n",
       "1            0.873016  \n",
       "2            0.575175  \n",
       "3            0.873016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Extraire les dates min/max\n",
    "query_min_max = \"\"\"\n",
    "SELECT MIN(order_purchase_timestamp) AS min_date,\n",
    "       MAX(order_purchase_timestamp) AS max_date\n",
    "FROM orders\n",
    "\"\"\"\n",
    "min_date, max_date = pd.read_sql_query(query_min_max, conn).iloc[0]\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = pd.to_datetime(max_date)\n",
    "\n",
    "# 2. Générer des bornes de périodes tous les 15 jours\n",
    "quinzaines = pd.date_range(start=min_date, end=max_date, freq='5D')\n",
    "\n",
    "# 3. Vérifier les périodes avec commandes\n",
    "df_dates = pd.read_sql_query(\"SELECT order_purchase_timestamp FROM orders\", conn)\n",
    "df_dates['order_purchase_timestamp'] = pd.to_datetime(df_dates['order_purchase_timestamp'])\n",
    "\n",
    "# 4. Générer le RFM pour chaque période de 15 jours\n",
    "rfm_all_periods = []\n",
    "\n",
    "for i in range(len(quinzaines) - 1):\n",
    "    start = quinzaines[i]\n",
    "    end = quinzaines[i + 1]\n",
    "\n",
    "    # Vérifier présence de commandes\n",
    "    has_orders = df_dates[\n",
    "        (df_dates['order_purchase_timestamp'] >= start) &\n",
    "        (df_dates['order_purchase_timestamp'] < end)\n",
    "    ]\n",
    "\n",
    "    if has_orders.empty:\n",
    "        continue\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH max_date AS (\n",
    "        SELECT DATE('{end.date()}') AS global_max_date\n",
    "    ),\n",
    "    customer_orders AS (\n",
    "        SELECT \n",
    "            c.customer_unique_id,\n",
    "            o.order_id,\n",
    "            o.order_purchase_timestamp\n",
    "        FROM customers c\n",
    "        JOIN orders o ON c.customer_id = o.customer_id\n",
    "        WHERE o.order_purchase_timestamp BETWEEN '{start.date()}' AND '{end.date()}'\n",
    "    ),\n",
    "    payments AS (\n",
    "        SELECT \n",
    "            co.customer_unique_id,\n",
    "            MAX(co.order_purchase_timestamp) AS last_order_date,\n",
    "            COUNT(DISTINCT co.order_id) AS frequency,\n",
    "            SUM(op.payment_value) AS monetary\n",
    "        FROM customer_orders co\n",
    "        JOIN order_pymts op ON co.order_id = op.order_id\n",
    "        GROUP BY co.customer_unique_id\n",
    "    ),\n",
    "    rfm AS (\n",
    "        SELECT \n",
    "            p.customer_unique_id,\n",
    "            CAST(JULIANDAY(m.global_max_date) - JULIANDAY(p.last_order_date) AS INT) AS recency,\n",
    "            p.frequency,\n",
    "            ROUND(p.monetary, 2) AS monetary\n",
    "        FROM payments p\n",
    "        CROSS JOIN max_date m\n",
    "    )\n",
    "\n",
    "    SELECT *, '{start.date()} → {end.date()}' AS period\n",
    "    FROM rfm\n",
    "    \"\"\"\n",
    "\n",
    "    df_rfm = pd.read_sql_query(query, conn)\n",
    "    rfm_all_periods.append(df_rfm)\n",
    "\n",
    "# Fusionner tous les RFM\n",
    "rfm_all_periods = pd.concat(rfm_all_periods, ignore_index=True)\n",
    "\n",
    "# 5. Évaluer la stabilité entre périodes consécutives\n",
    "periods = sorted(rfm_all_periods['period'].unique())\n",
    "results = []\n",
    "\n",
    "for i in range(len(periods) - 1):\n",
    "    period_1 = periods[i]\n",
    "    period_2 = periods[i + 1]\n",
    "\n",
    "    df1 = rfm_all_periods[rfm_all_periods['period'] == period_1].copy()\n",
    "    df2 = rfm_all_periods[rfm_all_periods['period'] == period_2].copy()\n",
    "\n",
    "    common_ids = set(df1.customer_unique_id).intersection(df2.customer_unique_id)\n",
    "    df1 = df1[df1.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "    df2 = df2[df2.customer_unique_id.isin(common_ids)].set_index(\"customer_unique_id\")\n",
    "\n",
    "    if len(common_ids) < 5:\n",
    "        continue  # ignorer si trop peu de clients communs\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(df1[['recency', 'frequency', 'monetary']])\n",
    "    X2 = scaler.fit_transform(df2[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "    # Clustering\n",
    "    kmeans_1 = KMeans(n_clusters=4, random_state=42).fit(X1)\n",
    "    kmeans_2 = KMeans(n_clusters=4, random_state=42).fit(X2)\n",
    "\n",
    "    dbscan_1 = DBSCAN(eps=1.0, min_samples=5).fit(X1)\n",
    "    dbscan_2 = DBSCAN(eps=1.0, min_samples=5).fit(X2)\n",
    "\n",
    "    # Stabilité des clusters\n",
    "    ari_kmeans = adjusted_rand_score(kmeans_1.labels_, kmeans_2.labels_)\n",
    "    ari_dbscan = adjusted_rand_score(dbscan_1.labels_, dbscan_2.labels_)\n",
    "\n",
    "    # Test KS pour la stabilité des distributions\n",
    "    ks_results = {\n",
    "        'recency': ks_2samp(df1['recency'], df2['recency']).pvalue,\n",
    "        'frequency': ks_2samp(df1['frequency'], df2['frequency']).pvalue,\n",
    "        'monetary': ks_2samp(df1['monetary'], df2['monetary']).pvalue,\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'period_pair': f\"{period_1} → {period_2}\",\n",
    "        'n_clients_common': len(common_ids),\n",
    "        'ARI_kmeans': ari_kmeans,\n",
    "        'ARI_dbscan': ari_dbscan,\n",
    "        'KS_pvalue_recency': ks_results['recency'],\n",
    "        'KS_pvalue_frequency': ks_results['frequency'],\n",
    "        'KS_pvalue_monetary': ks_results['monetary'],\n",
    "    })\n",
    "\n",
    "# 6. Résultat final\n",
    "df_eval = pd.DataFrame(results)\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78d17c-17eb-4e06-a140-fe73345beef8",
   "metadata": {},
   "source": [
    "| 🧠 Conclusion Globale                                         | ✅ Interprétation                                                                                                                                  |\n",
    "|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| ✔️ Distributions RFM stables                                 | Les tests de Kolmogorov-Smirnov montrent que les variables `recency`, `frequency` et `monetary` ne varient pas significativement entre périodes. |\n",
    "| ❌ Clusters (KMeans) instables                               | Les indices ARI sont faibles ou négatifs → faible cohérence des regroupements entre périodes.                                                     |\n",
    "| ➤ Hypothèse 1 : Échantillon trop petit                      | Peu de clients communs (5 à 7) → peu de données pour former des clusters fiables.                                                                 |\n",
    "| ➤ Hypothèse 2 : Comportement client trop changeant          | Les clients peuvent avoir des comportements très variables à l’échelle de 5 jours.                                                                |\n",
    "| ➤ Hypothèse 3 : Clustering inadapté                         | KMeans suppose des clusters sphériques et équilibrés, ce qui n’est peut-être pas le cas ici.                                                      |\n",
    "| ✔️ Clusters (DBSCAN) plus stables                            | DBSCAN montre une stabilité des regroupements (ARI = 1), ce qui indique une meilleure capacité à identifier des structures sous-jacentes.         |\n",
    "| ➤ Hypothèse 4 : DBSCAN détecte des structures non sphériques | Contrairement à KMeans, DBSCAN peut mieux gérer des structures complexes (clusters de formes variées) sans faire d'hypothèses sur leur forme.    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea726fe1-8efd-4a00-b4a2-572de6fb1a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e891adb-7715-42a4-bef5-51c0df115110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b62272-7a94-444d-b82f-40c5d365468d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Segmenter Des Clients (Poetry)",
   "language": "python",
   "name": "segmenter-des-clients-30gzothh-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
